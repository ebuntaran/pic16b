[
  {
    "objectID": "posts/homework-6/index.html",
    "href": "posts/homework-6/index.html",
    "title": "Fake News Classification with Keras",
    "section": "",
    "text": "In this tutorial, I will be demonstrating how to categorize fake news titles using Keras.\n\nSetting Up\nWe start with some of the necessary imports:\n\n!pip install keras --upgrade\n\n\nimport os\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\nimport keras\nfrom keras import layers, losses\nfrom keras.layers import TextVectorization\nimport pandas as pd\nimport tensorflow as tf\nfrom keras import utils\nfrom matplotlib import pyplot as plt\n\nWe can now import the data:\n\ntrain_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true\"\ndata = pd.read_csv(train_url)\n\n\n\nData Preparation\nWe construct the dataset we will use to train the model. Using the nltk stopwords list, we can remove stopwords from the dataset, and we can also convert all words to lowercase.\n\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\n\ndef make_dataset(data):\n    stop = stopwords.words('english')\n    for column in [\"title\", \"text\"]: #for each column...\n        data[column] = data[column].apply(lambda x: str.lower(x)) #convert string to lowercase\n        data[column] = data[column].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)])) #remove stopwords\n    dataset = tf.data.Dataset.from_tensor_slices( #make a tensorflow dataset\n        (\n            {\n                \"title\" : data[[\"title\"]], #input is a dictionary with corresponding title and text entries\n                \"text\" : data[[\"text\"]]\n            },\n            data[[\"fake\"]] #output is fake column\n        )\n    )\n    dataset.batch(100) #batching increases training speed\n    return dataset\n\n[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n\n\nThe dataset can be constructed by calling this function on the imported dataframe.\n\ndataset = make_dataset(data)\n\nWe split the dataset into three sets: The training set takes 60% of the data, the validation set takes 20% of the data, and the testing set takes another 20% of the data.\n\ndataset = dataset.shuffle(buffer_size = len(data), reshuffle_each_iteration=False)\ntrain_size = int(0.6*len(data))\nval_size = int(0.2*len(data))\n\ntrain = dataset.take(train_size)\nval = dataset.skip(train_size).take(val_size)\ntest = dataset.skip(train_size + val_size)\n\n\n\nBase Rate\nWe can determine the base rate by considering the proportion of entries with the most common label:\n\ncounter = 0\nfor _ , fake in train:\n    if fake == 1:\n        counter += 1\nprint(round(counter/train_size,2))\n\n0.52\n\n\nIn this case, a model that always outputs “fake” will have 52% accuracy.\n\n\nModel Preparation\nWe start by defining the text vectorization layer:\n\nsize_vocabulary = 2000\n\nvectorize_layer = TextVectorization(\n    max_tokens=size_vocabulary, # standardization isn't necessary because text has already been prepared in make_dataset\n    output_mode='int',\n    output_sequence_length=500)\nvectorize_layer.adapt(train.map(lambda x, y: x[\"title\"]))\n\nWe can then define the various other layers that will be used in each model:\n\ntitle_input = keras.Input(shape=(1,), name=\"title\", dtype=\"string\") #title input layer\ntext_input = keras.Input(shape=(1,), name=\"text\", dtype=\"string\") # text input layer\n\nembedding_layer = layers.Embedding(size_vocabulary, 10, name=\"embedding\")\n\n#title layers\ntitle_features = vectorize_layer(title_input)\ntitle_features = embedding_layer(title_features)\ntitle_features = layers.Dropout(0.2)(title_features)\n\ntitle_features = layers.GlobalAveragePooling1D()(title_features)\ntitle_features = layers.Dropout(0.2)(title_features)\n\ntitle_features = layers.Dense(32)(title_features)\ntitle_features = layers.Dropout(0.2)(title_features)\n\ntitle_features = layers.Dense(32, activation=\"relu\")(title_features)\n\n#text layers\ntext_features = vectorize_layer(text_input)\ntext_features = embedding_layer(text_features)\ntext_features = layers.Dropout(0.2)(text_features)\n\ntext_features = layers.GlobalAveragePooling1D()(text_features)\ntext_features = layers.Dropout(0.2)(text_features)\n\ntext_features = layers.Dense(32, activation=\"relu\")(text_features)\ntext_features = layers.Dropout(0.2)(text_features)\n\ntext_features = layers.Dense(32, activation=\"relu\")(text_features)\n\nfake_pred_title = layers.Dense(1, name=\"fake_title\")(title_features) #title output\nfake_pred_text = layers.Dense(1, name=\"fake_text\")(text_features) #text output\n\nmain = layers.concatenate([title_features, text_features], axis=1) #combine title, text streams\nfake_pred_both = layers.Dense(1, name=\"fake_both\")(main) #combined output\n\n\n\nModel 1: Title Only\nThe first model vectorizes the title text, runs it through an embedding layer, then a global_average_pooling layer, ending with two dense layers, with dropout layers interspersed to combat overfitting. I tried using an LSTM layer, but it was way too slow, even with a GPU.\n\nmodel1 = keras.Model(\n    inputs = title_input,\n    outputs = fake_pred_title\n)\n\nutils.plot_model(model1, \"model1.png\",\n                       show_shapes=True,\n                       show_layer_names=True,\n                        rankdir=\"LR\")\n\n\n\n\n\n\n\n\n\nmodel1.compile(\n    optimizer=\"adam\",\n    loss = losses.BinaryCrossentropy(from_logits=True),\n    metrics=[\"accuracy\"]\n)\n\ncallback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5) #combat overfitting with early stopping\nhistory1 = model1.fit(train, epochs = 50, validation_data = val, callbacks=[callback], verbose = True)\n\nplt.plot(history1.history[\"accuracy\"],label='training')\nplt.plot(history1.history[\"val_accuracy\"],label='validation')\nplt.legend()\n\nEpoch 1/50\n13469/13469 ━━━━━━━━━━━━━━━━━━━━ 26s 2ms/step - accuracy: 0.4802 - loss: 0.6954 - val_accuracy: 0.4778 - val_loss: 0.6925\nEpoch 2/50\n13469/13469 ━━━━━━━━━━━━━━━━━━━━ 19s 1ms/step - accuracy: 0.4789 - loss: 0.6933 - val_accuracy: 0.4778 - val_loss: 0.6922\nEpoch 3/50\n13469/13469 ━━━━━━━━━━━━━━━━━━━━ 23s 2ms/step - accuracy: 0.4792 - loss: 0.6929 - val_accuracy: 0.4778 - val_loss: 0.6882\nEpoch 4/50\n13469/13469 ━━━━━━━━━━━━━━━━━━━━ 41s 2ms/step - accuracy: 0.5122 - loss: 0.6756 - val_accuracy: 0.6280 - val_loss: 0.5536\nEpoch 5/50\n13469/13469 ━━━━━━━━━━━━━━━━━━━━ 41s 2ms/step - accuracy: 0.6755 - loss: 0.5721 - val_accuracy: 0.6683 - val_loss: 0.5100\nEpoch 6/50\n13469/13469 ━━━━━━━━━━━━━━━━━━━━ 41s 2ms/step - accuracy: 0.7124 - loss: 0.5281 - val_accuracy: 0.7866 - val_loss: 0.4155\nEpoch 7/50\n13469/13469 ━━━━━━━━━━━━━━━━━━━━ 19s 1ms/step - accuracy: 0.7472 - loss: 0.4902 - val_accuracy: 0.8138 - val_loss: 0.3655\nEpoch 8/50\n13469/13469 ━━━━━━━━━━━━━━━━━━━━ 23s 2ms/step - accuracy: 0.7798 - loss: 0.4467 - val_accuracy: 0.7677 - val_loss: 0.3965\nEpoch 9/50\n13469/13469 ━━━━━━━━━━━━━━━━━━━━ 20s 2ms/step - accuracy: 0.7954 - loss: 0.4193 - val_accuracy: 0.7930 - val_loss: 0.3669\nEpoch 10/50\n13469/13469 ━━━━━━━━━━━━━━━━━━━━ 20s 2ms/step - accuracy: 0.8180 - loss: 0.3883 - val_accuracy: 0.8695 - val_loss: 0.2797\nEpoch 11/50\n13469/13469 ━━━━━━━━━━━━━━━━━━━━ 18s 1ms/step - accuracy: 0.8263 - loss: 0.3691 - val_accuracy: 0.8093 - val_loss: 0.3015\nEpoch 12/50\n13469/13469 ━━━━━━━━━━━━━━━━━━━━ 26s 2ms/step - accuracy: 0.8277 - loss: 0.3556 - val_accuracy: 0.8187 - val_loss: 0.2964\nEpoch 13/50\n13469/13469 ━━━━━━━━━━━━━━━━━━━━ 18s 1ms/step - accuracy: 0.8405 - loss: 0.3421 - val_accuracy: 0.8115 - val_loss: 0.3081\nEpoch 14/50\n13469/13469 ━━━━━━━━━━━━━━━━━━━━ 21s 2ms/step - accuracy: 0.8474 - loss: 0.3283 - val_accuracy: 0.8568 - val_loss: 0.2549\nEpoch 15/50\n13469/13469 ━━━━━━━━━━━━━━━━━━━━ 20s 1ms/step - accuracy: 0.8556 - loss: 0.3197 - val_accuracy: 0.8476 - val_loss: 0.2523\nEpoch 16/50\n13469/13469 ━━━━━━━━━━━━━━━━━━━━ 19s 1ms/step - accuracy: 0.8570 - loss: 0.3140 - val_accuracy: 0.8325 - val_loss: 0.2865\nEpoch 17/50\n13469/13469 ━━━━━━━━━━━━━━━━━━━━ 22s 2ms/step - accuracy: 0.8616 - loss: 0.3076 - val_accuracy: 0.8260 - val_loss: 0.2872\nEpoch 18/50\n13469/13469 ━━━━━━━━━━━━━━━━━━━━ 39s 1ms/step - accuracy: 0.8637 - loss: 0.3033 - val_accuracy: 0.8425 - val_loss: 0.2722\nEpoch 19/50\n13469/13469 ━━━━━━━━━━━━━━━━━━━━ 20s 2ms/step - accuracy: 0.8724 - loss: 0.2887 - val_accuracy: 0.8334 - val_loss: 0.2853\nEpoch 20/50\n13469/13469 ━━━━━━━━━━━━━━━━━━━━ 20s 2ms/step - accuracy: 0.8729 - loss: 0.2861 - val_accuracy: 0.8287 - val_loss: 0.2804\n\n\n\n\n\n\n\n\n\nThe model performed decently, slowly stabilizing at around 83% accuracy.\n\n\nModel 2: Text Only\nThis model follows exactly the same structure as Model 1, except that it is fed only the text input instead of only the title input.\n\nmodel2 = keras.Model(\n    inputs = text_input,\n    outputs = fake_pred_text\n)\n\nutils.plot_model(model2, \"model2.png\",\n                       show_shapes=True,\n                       show_layer_names=True,\n                       rankdir=\"LR\")\n\n\n\n\n\n\n\n\n\nmodel2.compile(\n    loss = losses.BinaryCrossentropy(from_logits=True),\n    optimizer=\"adam\",\n    metrics=[\"accuracy\"]\n)\n\nhistory2 = model2.fit(train, epochs = 50, validation_data = val, callbacks=[callback], verbose = True)\n\nplt.plot(history2.history[\"accuracy\"],label='training')\nplt.plot(history2.history[\"val_accuracy\"],label='validation')\nplt.legend()\n\nEpoch 1/50\n13469/13469 ━━━━━━━━━━━━━━━━━━━━ 22s 2ms/step - accuracy: 0.7672 - loss: 0.4448 - val_accuracy: 0.9243 - val_loss: 0.2278\nEpoch 2/50\n13469/13469 ━━━━━━━━━━━━━━━━━━━━ 41s 2ms/step - accuracy: 0.9014 - loss: 0.2398 - val_accuracy: 0.9162 - val_loss: 0.1838\nEpoch 3/50\n13469/13469 ━━━━━━━━━━━━━━━━━━━━ 22s 2ms/step - accuracy: 0.9239 - loss: 0.1924 - val_accuracy: 0.9151 - val_loss: 0.1862\nEpoch 4/50\n13469/13469 ━━━━━━━━━━━━━━━━━━━━ 40s 2ms/step - accuracy: 0.9292 - loss: 0.1760 - val_accuracy: 0.9497 - val_loss: 0.1599\nEpoch 5/50\n13469/13469 ━━━━━━━━━━━━━━━━━━━━ 39s 1ms/step - accuracy: 0.9369 - loss: 0.1608 - val_accuracy: 0.9338 - val_loss: 0.1516\nEpoch 6/50\n13469/13469 ━━━━━━━━━━━━━━━━━━━━ 22s 2ms/step - accuracy: 0.9423 - loss: 0.1482 - val_accuracy: 0.9227 - val_loss: 0.1641\nEpoch 7/50\n13469/13469 ━━━━━━━━━━━━━━━━━━━━ 29s 2ms/step - accuracy: 0.9454 - loss: 0.1461 - val_accuracy: 0.9572 - val_loss: 0.1311\nEpoch 8/50\n13469/13469 ━━━━━━━━━━━━━━━━━━━━ 33s 2ms/step - accuracy: 0.9508 - loss: 0.1338 - val_accuracy: 0.9601 - val_loss: 0.1329\nEpoch 9/50\n13469/13469 ━━━━━━━━━━━━━━━━━━━━ 39s 1ms/step - accuracy: 0.9518 - loss: 0.1290 - val_accuracy: 0.9590 - val_loss: 0.1285\nEpoch 10/50\n13469/13469 ━━━━━━━━━━━━━━━━━━━━ 21s 1ms/step - accuracy: 0.9588 - loss: 0.1193 - val_accuracy: 0.9334 - val_loss: 0.1437\nEpoch 11/50\n13469/13469 ━━━━━━━━━━━━━━━━━━━━ 20s 1ms/step - accuracy: 0.9581 - loss: 0.1198 - val_accuracy: 0.9581 - val_loss: 0.1333\nEpoch 12/50\n13469/13469 ━━━━━━━━━━━━━━━━━━━━ 22s 2ms/step - accuracy: 0.9589 - loss: 0.1140 - val_accuracy: 0.9425 - val_loss: 0.1271\nEpoch 13/50\n13469/13469 ━━━━━━━━━━━━━━━━━━━━ 40s 2ms/step - accuracy: 0.9629 - loss: 0.1039 - val_accuracy: 0.9610 - val_loss: 0.1250\nEpoch 14/50\n13469/13469 ━━━━━━━━━━━━━━━━━━━━ 22s 2ms/step - accuracy: 0.9613 - loss: 0.1055 - val_accuracy: 0.9356 - val_loss: 0.1374\nEpoch 15/50\n13469/13469 ━━━━━━━━━━━━━━━━━━━━ 22s 2ms/step - accuracy: 0.9663 - loss: 0.0986 - val_accuracy: 0.9608 - val_loss: 0.1196\nEpoch 16/50\n13469/13469 ━━━━━━━━━━━━━━━━━━━━ 20s 2ms/step - accuracy: 0.9651 - loss: 0.0996 - val_accuracy: 0.9374 - val_loss: 0.1395\nEpoch 17/50\n13469/13469 ━━━━━━━━━━━━━━━━━━━━ 20s 1ms/step - accuracy: 0.9663 - loss: 0.0955 - val_accuracy: 0.9628 - val_loss: 0.1213\nEpoch 18/50\n13469/13469 ━━━━━━━━━━━━━━━━━━━━ 22s 2ms/step - accuracy: 0.9660 - loss: 0.0905 - val_accuracy: 0.9378 - val_loss: 0.1348\nEpoch 19/50\n13469/13469 ━━━━━━━━━━━━━━━━━━━━ 41s 2ms/step - accuracy: 0.9698 - loss: 0.0837 - val_accuracy: 0.9454 - val_loss: 0.1237\nEpoch 20/50\n13469/13469 ━━━━━━━━━━━━━━━━━━━━ 39s 1ms/step - accuracy: 0.9707 - loss: 0.0857 - val_accuracy: 0.9307 - val_loss: 0.1511\n\n\n\n\n\n\n\n\n\nThis model oscillated at around 94% accuracy. I was more or less expecting this result, since more data to work with usually results in better accuracy. Training was a lot loss stable than in Model 1, which is another expected consequence of working with a lot of data.\n\n\nModel 3: Title and Text\nThis model again functions similarly to the previous two models, except that it takes both inputs instead of just one. Both inputs are passed to the vectorization and embedding layers, then go through their global_average_pooling layer/dense/dropout layers before being concatenated, and ultimately ending with the dense classification layer.\n\nmodel3 = keras.Model(\n    inputs = [title_input, text_input],\n    outputs = fake_pred_both\n)\n\nutils.plot_model(model3, \"model3.png\",\n                       show_shapes=True,\n                       show_layer_names=True,\n                        rankdir=\"LR\") #graphviz doesn't like TB on this one for some reason\n\n\n\n\n\n\n\n\n\nmodel3.compile(\n    loss = losses.BinaryCrossentropy(from_logits=True),\n    optimizer=\"adam\",\n    metrics=[\"accuracy\"]\n)\n\nhistory3 = model3.fit(train, epochs = 50, validation_data = val, callbacks=[callback], verbose = True)\n\nplt.plot(history3.history[\"accuracy\"],label='training')\nplt.plot(history3.history[\"val_accuracy\"],label='validation')\nplt.legend()\n\nEpoch 1/50\n13469/13469 ━━━━━━━━━━━━━━━━━━━━ 27s 2ms/step - accuracy: 0.9642 - loss: 0.0920 - val_accuracy: 0.9434 - val_loss: 0.1449\nEpoch 2/50\n13469/13469 ━━━━━━━━━━━━━━━━━━━━ 40s 2ms/step - accuracy: 0.9727 - loss: 0.0803 - val_accuracy: 0.9679 - val_loss: 0.1003\nEpoch 3/50\n13469/13469 ━━━━━━━━━━━━━━━━━━━━ 24s 2ms/step - accuracy: 0.9733 - loss: 0.0724 - val_accuracy: 0.9599 - val_loss: 0.1201\nEpoch 4/50\n13469/13469 ━━━━━━━━━━━━━━━━━━━━ 40s 2ms/step - accuracy: 0.9726 - loss: 0.0795 - val_accuracy: 0.9661 - val_loss: 0.1018\nEpoch 5/50\n13469/13469 ━━━━━━━━━━━━━━━━━━━━ 42s 2ms/step - accuracy: 0.9751 - loss: 0.0715 - val_accuracy: 0.9728 - val_loss: 0.0928\nEpoch 6/50\n13469/13469 ━━━━━━━━━━━━━━━━━━━━ 24s 2ms/step - accuracy: 0.9771 - loss: 0.0663 - val_accuracy: 0.9688 - val_loss: 0.1068\nEpoch 7/50\n13469/13469 ━━━━━━━━━━━━━━━━━━━━ 42s 2ms/step - accuracy: 0.9780 - loss: 0.0632 - val_accuracy: 0.9735 - val_loss: 0.0854\nEpoch 8/50\n13469/13469 ━━━━━━━━━━━━━━━━━━━━ 39s 2ms/step - accuracy: 0.9778 - loss: 0.0618 - val_accuracy: 0.9724 - val_loss: 0.1120\nEpoch 9/50\n13469/13469 ━━━━━━━━━━━━━━━━━━━━ 41s 2ms/step - accuracy: 0.9767 - loss: 0.0685 - val_accuracy: 0.9733 - val_loss: 0.0961\nEpoch 10/50\n13469/13469 ━━━━━━━━━━━━━━━━━━━━ 41s 2ms/step - accuracy: 0.9802 - loss: 0.0565 - val_accuracy: 0.9733 - val_loss: 0.0925\nEpoch 11/50\n13469/13469 ━━━━━━━━━━━━━━━━━━━━ 41s 2ms/step - accuracy: 0.9782 - loss: 0.0610 - val_accuracy: 0.9722 - val_loss: 0.0979\nEpoch 12/50\n13469/13469 ━━━━━━━━━━━━━━━━━━━━ 24s 2ms/step - accuracy: 0.9795 - loss: 0.0580 - val_accuracy: 0.9764 - val_loss: 0.0863\n\n\n\n\n\n\n\n\n\nThis model performed better than both of the previous models, though not by much. The final model stabilized at around 97.2% accuracy.\nOverall, since Model 3 had the highest accuracy, it seems clear to me that algorithms detecting fake news should consider both the title and text of articles.\n\n\nAssessing Accuracy\n\nmodel3.evaluate(test)\n\n4491/4491 ━━━━━━━━━━━━━━━━━━━━ 4s 817us/step - accuracy: 0.9810 - loss: 0.0572\n\n\n[0.058508388698101044, 0.9806278944015503]\n\n\nModel 3 had 98% accuracy on the unseen testing data, which is pretty good!\nWe can also test it on another testing dataset:\n\ntest_url = \"https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true\"\ntest_data = pd.read_csv(test_url)\ntest_dataset = make_dataset(test_data)\nmodel3.evaluate(test_dataset)\n\n22449/22449 ━━━━━━━━━━━━━━━━━━━━ 21s 931us/step - accuracy: 0.9759 - loss: 0.0730\n\n\n[0.07201951742172241, 0.9764800071716309]\n\n\nWith this, we can be sure that our model can predict fake news with 97% accuracy.\n#Embedding Visualization\n\nimport plotly.io as pio\npio.renderers.default=\"iframe\"\n\n\nimport numpy as np\n\nweights = model3.get_layer('embedding').get_weights()[0] # get the weights from the embedding layer\nvocab = vectorize_layer.get_vocabulary()                # get the vocabulary from our data prep for later\n\nfrom sklearn.decomposition import PCA #reduce dimensionality to make visualization easier\npca = PCA(n_components=2)\nweights = pca.fit_transform(weights)\n\nembedding_df = pd.DataFrame({ #convert to dataframe\n    'word' : vocab,\n    'x0'   : weights[:,0],\n    'x1'   : weights[:,1]\n})\n\nimport plotly.express as px #make a scatterplot with plotly\nfig = px.scatter(embedding_df,\n                 x = \"x0\",\n                 y = \"x1\",\n                 size = list(np.ones(len(embedding_df))),\n                 size_max = 3,\n                 hover_name = \"word\")\nfig.show()\n\n\n\n\nOn the positive x0 side of the graph, hillary, obama's, and trump's are clear outliers. This may be because they are the names of important figures, who are more likely to appear in sensationalist headlines. On the negative x0 side of the graph are myanmar, catalan, russias, chinas, and turkey, among others. This may suggest that real news articles are more likely to contain more specific details, like geographic locations. The word says appears on the negative x0 side of the graph. This could be interpreted to mean that real news articles are more likely to cite their sources."
  },
  {
    "objectID": "posts/homework-4/index.html",
    "href": "posts/homework-4/index.html",
    "title": "Simulating Two-Dimensional Heat Diffusion with JAX and numpy",
    "section": "",
    "text": "In this blog post, I will be describing four different ways of simulating two-dimensional heat diffusion.\n\nGetting Started\nWe first import all of the necessary packages and initialize the starting grid.\n\nimport inspect\nimport time\nfrom matplotlib import pyplot as plt\nfrom heat_equation import *\n\nN = 101 # grid size\nepsilon = 0.2 # step size\niter = 2701 # number of iterations\n\n# construct initial condition: 1 unit of heat at midpoint.\nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\n\n\n\nMatrix multiplication\nWe construct a finite-differences matrix based on a discretization of the 2D heat equation in get_A.\n\nprint(inspect.getsource(get_A))\n\ndef get_A(N):\n    \"\"\"Returns the finite difference matrix A for advance_time_matvecmul\n    Args:\n        N: Size of the returned matrix\n\n    Returns:\n        N x N matrix A for advance_time_matvecmul\n    \"\"\"\n    n = N * N\n    diagonals = [-4 * np.ones(n), np.ones(n-1), np.ones(n-1), np.ones(n-N), np.ones(n-N)]\n    diagonals[1][(N-1)::N] = 0\n    diagonals[2][(N-1)::N] = 0\n    return np.diag(diagonals[0]) + np.diag(diagonals[1], 1) + np.diag(diagonals[2], -1) + np.diag(diagonals[3], N) + np.diag(diagonals[4], -N)\n\n\n\nWe can then call get_A in advance_time_matvecmul to generate the next step of the simulation via matrix-vector multiplcation with the flattened grid.\n\nprint(inspect.getsource(advance_time_matvecmul))\n\ndef advance_time_matvecmul(A, u, epsilon):\n    \"\"\"Advances the simulation by one timestep, via matrix-vector multiplication\n    Args:\n        A: The 2d finite difference matrix\n        u: N x N grid state at timestep k\n        epsilon: stability constant\n\n    Returns:\n        N x N Grid state at timestep k+1\n    \"\"\"\n    N = u.shape[0]\n    u = u + epsilon * (A @ u.flatten()).reshape((N, N))\n    return u\n\n\n\nWith these two functions, we can run the simulation for 2700 timesteps and plot every 300th iteration. The execution time for calculating the timesteps can also be displayed by using the time module. We will repeat this segment for each simulation method so that we can compare them.\n\nu = u0 # make a copy of u0\nfigs = [None]*9 # list of in-progress figures\nstart_time = time.time()\n\nA = get_A(N) # get the finite difference matrix\nfor i in range(1,iter+1):\n    u = advance_time_matvecmul(A, u, epsilon) # advance by one timestep\n    if i % 300 == 0:\n        figs[(i//300) - 1] = u # save every 300th iteration\nprint(\"--- %s seconds ---\" % (time.time() - start_time)) # print execution time\n\nfig, axs = plt.subplots(3,3)\nfor i in range(9):\n    axs[i//3][i%3].imshow(figs[i]) # plot in-progress figures\n\n--- 89.75409722328186 seconds ---\n\n\n\n\n\n\n\n\n\n\n\nMatrix Multiplication with JAX Sparse Matrix\nSince many of the entries in matrix A are zeroes, we can use a sparse matrix to improve execution time. We accomplish this by calling get_A and converting the returned matrix to a sparse matrix.\n\nprint(inspect.getsource(get_sparse_A))\n\ndef get_sparse_A(N):\n    \"\"\"Returns the sparse matrix A for advance_time_matvecmul_jax\n    Args:\n        N: Size of the returned matrix\n\n    Returns:\n        N x N sparsematrix A for advance_time_matvecmul\n    \"\"\"\n    # call get_A to get A, convert it to a jax array, then convert that to a sparse matrix\n    return sparse.BCOO.fromdense(jnp.array(get_A(N)))\n\n\n\n\nu = u0\nfigs = [None]*9\nstart_time = time.time()\n\nA = get_sparse_A(N)\nfor i in range(1,iter+1):\n    u = advance_time_matvecmul_jax(A, u, epsilon)\n    if i % 300 == 0:\n        figs[(i//300) - 1] = u\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\n\nfig, axs = plt.subplots(3,3)\nfor i in range(9):\n    axs[i//3][i%3].imshow(figs[i])\n\n--- 7.294459819793701 seconds ---\n\n\n\n\n\n\n\n\n\n\n\nDirect Operation with Numpy\nIt turns out that in this particular scenario, directly calculation with numpy vectorized operations is faster than matrix multiplication. We first pad the matrix with zeroes all around to simulate the boundary conditions. After flattening the matrix, we can use np.roll to align all of the necessary cells needed to compute each cell of the simulation. After computing the next timestep, we can reshape the matrix back into a square and return the non-boundary part of the simulation.\n\nprint(inspect.getsource(advance_time_numpy))\n\ndef advance_time_numpy(u, alpha):\n    \"\"\"Advances the simulation by one timestep, via numpy techniques\n    Args:\n        u: N x N grid state at timestep k\n        alpha: stability constant\n\n    Returns:\n        N x N Grid state at timestep k+1\n    \"\"\"\n    #Get N (grid size)\n    N = u.shape[0]\n\n    #pad u with 0s to represent boundary conditions and flatten\n    u_old = np.pad(u,((1,1), (1,1)), constant_values=0).flatten()\n\n\n    u_new = u_old + alpha*(np.roll(u_old,N+2) # u_i-1,j\n                           + np.roll(u_old,-(N+2)) # u_i+1,j\n                           + np.roll(u_old,-1) # u_i,j+1\n                           + np.roll(u_old,1) # u_i,j-1\n                           - 4*u_old) #4 * u_i,j\n    \n    #reshape array into a square then return only the portion that is part of the simulation\n    return np.reshape(u_new,(N+2,N+2))[1:N+1,1:N+1]\n\n\n\n\nu = u0\nfigs = [None]*9\nstart_time = time.time()\n\nfor i in range(1,iter+1):\n    u = advance_time_numpy(u, epsilon)\n    if i % 300 == 0:\n        figs[(i//300) - 1] = u\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\n\nfig, axs = plt.subplots(3,3)\nfor i in range(9):\n    axs[i//3][i%3].imshow(figs[i])\n\n--- 0.37366628646850586 seconds ---\n\n\n\n\n\n\n\n\n\n\n\nDirect Operation with JAX\nWe can use just-in-time compilation to speed up the computation by replacing all numpy functions with their jax_numpy equivalents.\n\nprint(inspect.getsource(advance_time_jax))\n\n@jit\ndef advance_time_jax(u, alpha):\n    #essentially the same as advance_time_numpy, but uses jax numpy functions instead\n    N = u.shape[0]\n    u_old = jnp.pad(u,((1,1), (1,1)), constant_values=0).flatten()\n    u_new = u_old + alpha*(jnp.roll(u_old,N+2)\n                           + jnp.roll(u_old,-(N+2))\n                           + jnp.roll(u_old,-1)\n                           + jnp.roll(u_old,1)\n                           - 4*u_old)\n    return jnp.reshape(u_new,(N+2,N+2))[1:N+1,1:N+1]\n\n\n\n\nu = u0\nfigs = [None]*9\nstart_time = time.time()\n\nfor i in range(1,iter+1):\n    u = advance_time_jax(u, epsilon)\n    if i % 300 == 0:\n        figs[(i//300) - 1] = u\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\n\nfig, axs = plt.subplots(3,3)\nfor i in range(9):\n    axs[i//3][i%3].imshow(figs[i])\n\n--- 0.17978262901306152 seconds ---\n\n\n\n\n\n\n\n\n\n\n\nComparison of implementations\nThe first method (matrix multiplication) was the easiest to code (since most of the code was already provided by professor) but it was also the slowest by far (taking 89.7 seconds). The second method with sparse matrices provided a significant speedup (taking only 7.2 seconds), and it wasn’t that much work to modify the first method to code it. The third method with direct operation was the easiest for me to understand since it draws directly on the differential equation, and it also provided another dramatic decrease in execution time (taking 0.37 seconds). The final method was again easy to derive from the third, and it provided a moderate speed boost from the already-fast third method (taking 0.18 seconds). Overall, if I had to code this again from scratch, I would definitely go with the fourth method."
  },
  {
    "objectID": "posts/homework-2/index.html",
    "href": "posts/homework-2/index.html",
    "title": "Using Scrapy for Movie Recommendations",
    "section": "",
    "text": "In this tutorial, I will be creating a web crawler with scrapy, which I will use to scrape TMDB in order to make a primitive movie recommendation system by counting the number of shared actors with various films/TV shows. (The repo for this project can be found here)\n\nSetup\nStart by opening a terminal and typing the following commands (assuming you already have conda installed with scrapy):\nconda activate Environment_Name\nscrapy startproject TMDB_scraper\ncd TMDB_scraper\nThis will create a folder called TMDB_scraper in your current directory. Navigate to /TMND_scraper/TMND_scraper/settings.py and set the USER_AGENT variable to a user agent of your choosing. This will help bypass potential Error 403s later.\n\n\nWriting the scraper\nCreate a file in the /TMND_scraper/TMND_scraper/spiders directory called tmdb_spider.py. This is where the most of scraper behavior will be held.\nThen, add the following code block to the beginning of the file:\nimport scrapy\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n    def __init__(self, subdir=None, *args, **kwargs):\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\nThis defines the basic structure of the scraper. Next, add the following three methods to the file:\ndef parse(self, response):\n    \"\"\"\n    Parses TMDB movie pages.\n    Yields a scrapy.Request for the \"Full Cast & Crew\" page, with the parse_full_credit method specified in the callback argument.\n    Does not return any data.\n    \"\"\"\n    yield scrapy.Request(response.url+\"/cast/\", callback = self.parse_full_credits)\nAs the docstring states, this method takes a TMDB movie page and navigates to the “Full Cast & Crew” page by just appending “/cast/” to the url.\nThis page is then passed into the parse_full_credits method, defined below:\ndef parse_full_credits(self, response):\n    \"\"\"\n    Parses TMDB \"Full Cast & Crew\" pages.\n    Yields a scrapy.Request for the page of each cast member, with the parse_actor_page method specified in the callback argument.\n    Does not return any data.\n    \"\"\"\n    #Select only the Cast section\n    cast = response.css(\"ol.people.credits\")[0]\n    #Select actor names\n    for href in cast.css(\"p a::attr(href)\").getall():\n        yield scrapy.Request(response.urljoin(href), callback = self.parse_actor_page)\nThis method takes the “Full Cast & Crew” page and navigates to the cast members’ pages. It does this by selecting a css element containing the details of the cast members, then selecting all of the links in that section. Note the [0] index on the cast object! Leaving this off would select crew members as well, which is not what we want.\nThe pages from this method are passed into the parse_actor_pages method, defined below:\ndef parse_actor_page(self, response):\n    \"\"\"\n    Parses TMDB actor pages.\n    Yields a dictionary containing the actor's name keyed by \"actor\" and the movie/TV show keyed by \"movie_or_TV_name\" for each movie/TV show they have acted in.\n    \"\"\"\n    #Select actor name\n    actor = response.css(\"h2.title a::text\").get()\n\n    #Figure out which section is the \"Acting\" section by examining all the headers\n    acting_index = response.css(\"div.credits_list h3::text\").getall().index(\"Acting\")\n    #Select all acting roles\n    acting = response.css(\"div.credits_list table.card.credits\")[acting_index]\n    for title in acting.css(\"a.tooltip bdi::text\"):\n        yield {\"actor\": actor, \"movie_or_TV_name\": title.get()}\nFirst, this method finds the actor’s name on the page. This is easily accomplished by looking for the h2 css tag and taking the text under it.\nHowever, finding the sections with their acting roles is a little trickier. We first select all of the h3 headers under div.credits_list, looking for the one that says “Acting”. Then, we store the index and use it to select the corresponding table under div.credits_list. From there, we can easily select all of the titles. For the output, we yield a dictionary consisting of the actor’s name and the movie/TV series name for each acting credit.\n\n\nRunning the scraper\nOnce all of this is done, running the scraper is a relatively simple task. Simply type into the terminal:\nscrapy crawl tmdb_spider -o results.csv -a subdir=9323-ghost-in-the-shell\nOf course, you can replace the subdir argument with whatever movie you like. This command will produce a csv file in /TMDB_scraper/results.csv containing all of the actor/movie information for the given movie.\n\n\nReading the data\nWe can just use pandas to read the results from the results.csv file.\n\nimport pandas as pd\ndf = pd.read_csv(\"TMDB_scraper/results.csv\")\n\nWe can use pandas’s .value_counts() method to count the number of times each movie appears in the dataframe. This returns a series, so we need to do some manipulation to get it into a dataframe. We also rename the columns to make them look more professional for plotting.\n\ncounts = df[\"movie_or_TV_name\"].value_counts()\ncounts = counts.to_frame()\ncounts = counts.reset_index()\ncounts.rename(columns={\"movie_or_TV_name\": \"Movie or TV show name\",\"count\": \"Number of shared actors\"},inplace=True)\n\ncounts.head()\n\n\n\n\n\n\n\n\nMovie or TV show name\nNumber of shared actors\n\n\n\n\n0\nGhost in the Shell\n23\n\n\n1\nCowboy Bebop\n9\n\n\n2\nMonster\n8\n\n\n3\nBlack Jack\n6\n\n\n4\nGhost in the Shell: Stand Alone Complex\n6\n\n\n\n\n\n\n\n\n\nPlotting the data\nWe use plotly with the data, making a bar chart of the top 10 movies/TV shows sharing actors with Ghost in the Shell in this case.\n\nimport plotly.io as pio\npio.renderers.default=\"iframe\" #For rendering on the blog\nfrom plotly import express as px\n\nfig = px.bar(counts.head(10),\n                x=\"Movie or TV show name\",\n                y=\"Number of shared actors\",\n                title = \"Top 10 Movies/TV Shows with most actors in common with Ghost in the Shell\")\nfig.show()\n\n\n\n\nUnsurprisingly, Ghost in the Shell has the most actors in common with Ghost in the Shell. It’s also unsurprising that the various spin-offs of Ghost in the Shell also share a few actors. Cowboy Bebop, Monster, and Black Jack are all anime that ran during the same era, so it makes sense that they would share actors."
  },
  {
    "objectID": "posts/homework-0/index.html",
    "href": "posts/homework-0/index.html",
    "title": "Homework 0",
    "section": "",
    "text": "In this blog post, I will be describing how to make a simple line plot of the Palmer Penguins dataset.\n\nImporting packages and getting data\nTo start, we first import pandas in order to download and read the dataset.\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\n\nTo help with plotting, we import matplotlib and seaborn (a library for matplotlib).\n\nimport seaborn as sns \nfrom matplotlib import pyplot as plt\nimport numpy as np\n\n\n\nSelecting Data\nWe select only the subset of data that pertains to Adelie penguins.\n\nadelie = penguins[penguins[\"Species\"] == \"Adelie Penguin (Pygoscelis adeliae)\"]\n\n\n\nPlotting Data\nFinally, we can plot the data using seaborn’s lineplot function by specifying the dataset and the axes we wish to plot against.\n\nsns.lineplot(data = adelie, x = \"Flipper Length (mm)\", y = \"Body Mass (g)\").set(title=\"Flipper Length (mm) vs Body Mass (g) for Adelie Penguins\")"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PIC 16B Blog",
    "section": "",
    "text": "Fake News Classification with Keras\n\n\n\n\n\n\n\n\n\n\n\nMar 5, 2024\n\n\nEthan Buntaran\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Dogs and Cats with Keras\n\n\n\n\n\n\n\n\n\n\n\nMar 1, 2024\n\n\nEthan Buntaran\n\n\n\n\n\n\n\n\n\n\n\n\nSimulating Two-Dimensional Heat Diffusion with JAX and numpy\n\n\n\n\n\n\n\n\n\n\n\nFeb 19, 2024\n\n\nEthan Buntaran\n\n\n\n\n\n\n\n\n\n\n\n\nA Simple Message Bank With Flask\n\n\n\n\n\n\n\n\n\n\n\nFeb 12, 2024\n\n\nEthan Buntaran\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Scrapy for Movie Recommendations\n\n\n\n\n\n\n\n\n\n\n\nJan 30, 2024\n\n\nEthan Buntaran\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of Temperature Data for Countries\n\n\n\n\n\n\n\n\n\n\n\nJan 27, 2024\n\n\nEthan Buntaran\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 0\n\n\n\n\n\n\n\n\n\n\n\nJan 20, 2024\n\n\nEthan Buntaran\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/homework-1/index.html",
    "href": "posts/homework-1/index.html",
    "title": "Analysis of Temperature Data for Countries",
    "section": "",
    "text": "#Included to make the plots interactive\nimport plotly.io as pio\npio.renderers.default=\"iframe\"\n\n\nSetting Up the Database\nMost of this code is taken from the lecture “Working with Datasets”:\nWe first clean up the dataframes, then add them as databases using sqlite3.\n\nwith open(\"database_setup.py\", 'r') as f:\n    print(f.read())\n\nimport sqlite3\nimport pandas as pd\n\n#Mostly taken from the lecture \"Working with Datasets\"\ndef prepare_df(df):\n    df = df.set_index(keys=[\"ID\", \"Year\"])\n    df = df.stack()\n    df = df.reset_index()\n    df = df.rename(columns = {\"level_2\"  : \"Month\" , 0 : \"Temp\"})\n    df[\"Month\"] = df[\"Month\"].str[5:].astype(int)\n    df[\"Temp\"]  = df[\"Temp\"] / 100\n    return(df)\n\nwith sqlite3.connect(\"temps.db\") as conn: # create a database in current directory called temps.db\n    temps_iter = pd.read_csv(\"temps.csv\", chunksize = 100000)\n    for i, df in enumerate(temps_iter):\n        df = prepare_df(df)\n        df.to_sql(\"temperatures\", conn, if_exists = \"replace\" if i == 0 else \"append\", index = False)\n    url = \"station-metadata.csv\"\n    stations = pd.read_csv(url)\n    stations.to_sql(\"stations\", conn, if_exists = \"replace\", index=False)\n    countries = pd.read_csv(\"https://raw.githubusercontent.com/mysociety/gaze/master/data/fips-10-4-to-iso-country-codes.csv\")\n    countries.to_sql(\"countries\", conn, if_exists = \"replace\", index=False)\n\n\n\n\nQuerying the Database\nThe vast majority of the query_climate_database function is a single SQL query. We select the relevant columns and only take rows satisfying the given constraints. Since country and station data is not in the temperatures database, we need to do some merges in order to get all the relevant information into one table.\n\nimport inspect\nfrom climate_database import query_climate_database\nprint(inspect.getsource(query_climate_database))\n\ndef query_climate_database(db_file, country, year_begin, year_end, month):\n    \"\"\"\n    Query a climate database for temperature information\n\n    Args:\n        db_file: sqlite3 database file\n        country (str): Country to take temperature data\n        year_begin (int): Year to begin taking data\n        year_end (int): Year to stop taking data\n        month (int): Month to take temperature data\n    \n    Returns:\n        df: Dataframe of temperature data at the provided country in the provided time range\n    \"\"\"\n\n    cmd = \\\n    f\"\"\"\n    SELECT S.name, S.latitude, S.longitude, C.Name, T.year, T.month, T.temp \n    FROM temperatures T\n    LEFT JOIN stations S ON T.id = S.id\n    INNER JOIN countries C ON SUBSTR(S.id,1,2) = C.\"FIPS 10-4\"\n    WHERE C.Name = \"{country}\" AND T.month = {month} AND T.year &gt;= {year_begin} AND T.year &lt;= {year_end}\n    \"\"\"\n    with sqlite3.connect(db_file) as conn:\n        df = pd.read_sql_query(cmd, conn)\n        df.rename(columns={\"Name\": \"Country\"},inplace=True)\n    return df\n\n\n\nHere is an example of this function in action:\n\nquery_climate_database(db_file = \"temps.db\",\n                       country = \"India\", \n                       year_begin = 1980, \n                       year_end = 2020,\n                       month = 1)\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nCountry\nYear\nMonth\nTemp\n\n\n\n\n0\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1980\n1\n23.48\n\n\n1\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1981\n1\n24.57\n\n\n2\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1982\n1\n24.19\n\n\n3\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1983\n1\n23.51\n\n\n4\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1984\n1\n24.81\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3147\nDARJEELING\n27.050\n88.270\nIndia\n1983\n1\n5.10\n\n\n3148\nDARJEELING\n27.050\n88.270\nIndia\n1986\n1\n6.90\n\n\n3149\nDARJEELING\n27.050\n88.270\nIndia\n1994\n1\n8.10\n\n\n3150\nDARJEELING\n27.050\n88.270\nIndia\n1995\n1\n5.60\n\n\n3151\nDARJEELING\n27.050\n88.270\nIndia\n1997\n1\n5.70\n\n\n\n\n3152 rows × 7 columns\n\n\n\n\n\nPlotting the Data\nIn order to first filter out stations that do not have enough observation data, we add a column showing the number of years each sation has been providing data for using transform, which we can then use to filter out those rows.\nWith transform, we can estimate the yearly temperature increase using linear regression. Finally, we use plotly’s scatter_mapbox function to make a plot of the data.\n\nimport plotly\nfrom plotly import express as px\nimport numpy as np\nimport calendar\nfrom sklearn.linear_model import LinearRegression\n\ndef temperature_coefficient_plot(db_file, country, year_begin, year_end, month, min_obs, **kwargs):\n    \"\"\"\n    Use a climate database to make a geographic scatterplot\n\n    Args:\n        db_file: sqlite3 database file\n        country (str): Country to take temperature data\n        year_begin (int): Year to begin taking data\n        year_end (int): Year to stop taking data\n        month (int): Month to take temperature data\n        min_obs (int): Minimum number of years with recordings for stations\n        **kwargs: Additional arguments to be passed into scatter_mapbox \n    Returns:\n        fig: Geographic scatterplot of data\n    \"\"\"\n\n    #collect data\n    df = query_climate_database(db_file, country, year_begin, year_end, month)\n    \n    # we don't need the month and country columns anymore since month and country are arguments of the function\n    df = df.drop([\"Month\",\"Country\"], axis=1)\n\n    # filter out stations with range of years of operation less than min_obs\n    df[\"obs\"] = df.groupby([\"NAME\", \"LATITUDE\", \"LONGITUDE\"])[\"Year\"].transform(np.ptp)\n    df = df[df[\"obs\"]&gt;min_obs]\n\n    # defined in the lecture \"Advanced Data Manipulation II\": uses linear regression to calculate the estimated yearly increase\n    def coef(data_group):\n        x = data_group[[\"Year\"]] # 2 brackets because X should be a df\n        y = data_group[\"Temp\"]   # 1 bracket because y should be a series\n        LR = LinearRegression()\n        LR.fit(x, y)\n        return LR.coef_[0]\n\n    # add a column for the yearly increase\n    df = df.groupby([\"NAME\", \"LATITUDE\", \"LONGITUDE\"]).apply(coef)\n    df = df.reset_index()\n    df[0] = df[0].round(4)\n    df.rename(columns={0: \"Estimated Yearly Increase (°C)\"},inplace=True)\n\n    # create the plot with all the gathered information\n    fig = px.scatter_mapbox(df, \n                            title = f\"Estimates of yearly increase in temperature in {calendar.month_name[month]}&lt;br&gt;for stations in {country}, years {year_begin} - {year_end}\",\n                            hover_name = \"NAME\",\n                            lat = \"LATITUDE\",\n                            lon = \"LONGITUDE\", \n                            color = \"Estimated Yearly Increase (°C)\",\n                            **kwargs)\n    fig.update_coloraxes(cmid=0)\n    return fig\n\nWe can now call the functions with various arguments to see what the yearly increases in temperature are across each country.\n\ncolor_map = px.colors.diverging.RdGy_r\n\nfig1 = temperature_coefficient_plot(\"temps.db\", \"India\", 1980, 2020, 1, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\n\nfig1.show()\n\n\n\n\n\nfig2 = temperature_coefficient_plot(\"temps.db\", \"France\", 1980, 2020, 1, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\n\nfig2.show()\n\n\n\n\n\n\nPlotting Variation in Yearly Increase\nWe can see by the color scale that there can be significant variation of yearly increase within a country, but exactly how much is there? We can use the same data to make a box plot of yearly increases throughout a country, again using plotly.\n\nimport pandas as pd\n\ndef temperature_box_plot(db_file, countries, year_begin, year_end, month, min_obs):\n    \"\"\"\n    Use a climate database to make a box plot\n\n    Args:\n        db_file: sqlite3 database file\n        countries (list): Countries to take temperature data\n        year_begin (int): Year to begin taking data\n        year_end (int): Year to stop taking data\n        month (int): Month to take temperature data\n        min_obs (int): Minimum number of years with recordings for stations\n    Returns:\n        fig: Box plot of data\n    \"\"\"\n    df = pd.DataFrame()\n    #collect data\n    for country in countries:\n        df = pd.concat([df,query_climate_database(db_file, country, year_begin, year_end, month)], ignore_index = True)\n    \n    # we don't need the month column anymore since month is an argument of the function\n    df = df.drop([\"Month\"], axis=1)\n\n    # filter out stations with range of years of operation less than min_obs\n    df[\"obs\"] = df.groupby([\"NAME\", \"LATITUDE\", \"LONGITUDE\", \"Country\"])[\"Year\"].transform(np.ptp)\n    df = df[df[\"obs\"]&gt;min_obs]\n\n    # defined in the lecture \"Advanced Data Manipulation II\": uses linear regression to calculate the estimated yearly increase\n    def coef(data_group):\n        x = data_group[[\"Year\"]] # 2 brackets because X should be a df\n        y = data_group[\"Temp\"]   # 1 bracket because y should be a series\n        LR = LinearRegression()\n        LR.fit(x, y)\n        return LR.coef_[0]\n\n    # add a column for the yearly increase\n    df = df.groupby([\"NAME\", \"LATITUDE\", \"LONGITUDE\", \"Country\"]).apply(coef)\n    df = df.reset_index()\n    df[0] = df[0].round(4)\n    df.rename(columns={0: \"Estimated Yearly Increase (°C)\"},inplace=True)\n\n    # create the plot with all the gathered information\n    fig = px.box(df, \"Estimated Yearly Increase (°C)\", facet_col=\"Country\",\n                 title=f\"Variation in yearly increase in temperature in {calendar.month_name[month]}&lt;br&gt;for stations in {country}, years {year_begin} - {year_end}\",)\n    return fig\n\n\nfig3 = temperature_box_plot(\"temps.db\", [\"India\",\"China\",\"France\"], 1980, 2020, 1, min_obs = 10)\nfig3.show()\n\n\n\n\n\n\nIs Yearly Increase Correlated With Latitude?\nWe’ve seen that yearly increase can vary significantly within a country, so could this be tied to the latitude of the stations?\nTo answer this question, we first need another SQL querying function.\nquery_climate_database_long accesses the same information as query_climate_database, but it instead takes stations that are within tol degrees of a given longitude (long).\n\nfrom climate_database import query_climate_database_long\nimport inspect\nprint(inspect.getsource(query_climate_database_long))\n\ndef query_climate_database_long(db_file, long, tol, year_begin, year_end, month):\n    \"\"\"\n    Query a climate database for temperature information\n\n    Args:\n        db_file: sqlite3 database file\n        long (int): Longitude to take data from\n        tol (int): Tolerance for longitude value\n        year_begin (int): Year to begin taking data\n        year_end (int): Year to stop taking data\n        month (int): Month to take temperature data\n    \n    Returns:\n        df: Dataframe of temperature data near the provided longitude in the provided time range\n    \"\"\"\n\n    cmd = \\\n    f\"\"\"\n    SELECT S.name, S.latitude, S.longitude, T.year, T.month, T.temp \n    FROM temperatures T\n    LEFT JOIN stations S ON T.id = S.id\n    WHERE T.month = {month} AND T.year &gt;= {year_begin} AND T.year &lt;= {year_end} AND S.longitude &lt; {long+tol} AND S.longitude &gt; {long-tol}\n    \"\"\"\n    with sqlite3.connect(db_file) as conn:\n        df = pd.read_sql_query(cmd, conn)\n    return df\n\n\n\nWe can now use this function to gather data and plot it, again using plotly.\n\ndef temperature_longitude_plot(db_file, long, tol, year_begin, year_end, month, min_obs, **kwargs):\n    \"\"\"\n    Use a climate database to make a geographic scatterplot\n\n    Args:\n        db_file: sqlite3 database file\n        long (int): Longitude to take data from\n        tol (int): Tolerance for longitude value\n        year_begin (int): Year to begin taking data\n        year_end (int): Year to stop taking data\n        month (int): Month to take temperature data\n        min_obs (int): Minimum number of years with recordings for stations\n        **kwargs: Additional arguments to be passed into scatter \n    Returns:\n        fig: Scatterplot of data\n    \"\"\"\n    \n    #collect data\n    df = query_climate_database_long(db_file, long, tol, year_begin, year_end, month)\n    \n    # we don't need the month column anymore since month is an argument of the function\n    df = df.drop([\"Month\"], axis=1)\n\n    # filter out stations with range of years of operation less than min_obs\n    df[\"obs\"] = df.groupby([\"NAME\", \"LATITUDE\", \"LONGITUDE\"])[\"Year\"].transform(np.ptp)\n    df = df[df[\"obs\"]&gt;min_obs]\n\n    # defined in the lecture \"Advanced Data Manipulation II\": uses linear regression to calculate the estimated yearly increase\n    def coef(data_group):\n        x = data_group[[\"Year\"]] # 2 brackets because X should be a df\n        y = data_group[\"Temp\"]   # 1 bracket because y should be a series\n        LR = LinearRegression()\n        LR.fit(x, y)\n        return LR.coef_[0]\n\n    # add a column for the yearly increase\n    df = df.groupby([\"NAME\", \"LATITUDE\", \"LONGITUDE\"]).apply(coef)\n    df = df.reset_index()\n    df[0] = df[0].round(4)\n    df.rename(columns={0: \"Estimated Yearly Increase (°C)\", \"LATITUDE\": \"Latitude (°)\"},inplace=True)\n    \n    # create the plot with all the gathered information\n    fig = px.scatter(df, \n                    title = f\"Estimates of yearly increase in temperature in {calendar.month_name[month]}&lt;br&gt;for longitudes {long-tol}° to {long+tol}°, years {year_begin} - {year_end}\",\n                    hover_name = \"NAME\",\n                    x = \"Latitude (°)\",\n                    y = \"Estimated Yearly Increase (°C)\",\n                    **kwargs)\n    return fig\n\n\nfig4 = temperature_longitude_plot(\"temps.db\", 0, 5, 1980, 2020, 1, \n                                   min_obs = 10)\nfig4.show()"
  },
  {
    "objectID": "posts/homework-3/index.html",
    "href": "posts/homework-3/index.html",
    "title": "A Simple Message Bank With Flask",
    "section": "",
    "text": "In this post I will be going through some of the functions and templates I used to create a simple online message bank using flask.\nThe repository for this project can be found here: https://github.com/ebuntaran/pic16b-mnist-demo/tree/fork\n\nHelper functions\nAll of these functions are defined in the app.py file, which contains most of the functionality of the website.\nThe first function (get_message_db) simply returns a database of messages if one exists. If no such database exists, it creates one with a table called messages with columns called handle and message using a SQL query .\ndef get_message_db():\n#check if there is a message database. if not...\n  try:\n      return g.message_db\n  except:\n    #create a new database\n    g.message_db = sqlite3.connect(\"messages_db.sqlite\")\n    #add a table with columns for handles and messages\n    cmd = 'CREATE TABLE IF NOT EXISTS messages (handle text(255), message text(255))'\n    cursor = g.message_db.cursor()\n    cursor.execute(cmd)\n    return g.message_db\nNow that we have a way to create/access a database, we need a way to add to the database. The insert_message message takes handle and message arguments, and uses these to add a row to the database using a SQL query:\ndef insert_message(handle, message):\n        #get database\n        db = get_message_db()\n        cursor = db.cursor()\n        #add a new row to the database with the given info with a sql command\n        cmd = \\\n        f\"\"\"\n        INSERT INTO messages (handle, message)\n        VALUES (\"{handle}\", \"{message}\");\n        \"\"\"\n        cursor.execute(cmd)\n        db.commit()\nNow all that’s left is a function to actually read the database. The random_messages accesses the database and selects at most n random messages from the database, returning them as a pandas dataframe:\ndef random_messages(n):\n        #select n random messages from the database with a sql command\n        cmd = \\\n        f\"\"\"\n        SELECT * FROM messages ORDER BY RANDOM() LIMIT {n};\n        \"\"\"\n        with get_message_db() as conn:\n            messages = pd.read_sql_query(cmd, conn)\n        #return as a dataframe\n        return messages\n\n\nSite functions\nNow that we have defined the helper functions, we can use them with the site rendering functions.\nThe first function is for the submission page. For GET requests, the site displays a form with input boxes for handle and message. For POST requests, the site uses the insert_message to add the information to the database, then displays a thank you message if it was successful:\n@app.route('/submit/', methods=['POST', 'GET'])\ndef submit():\n    if request.method == 'GET':\n        return render_template('submit.html')\n    else:\n        try:\n            #call the insert message function with the parameters given by the form\n            insert_message(request.form[\"handle\"], request.form[\"message\"])\n            #show a message if the request goes through\n            return render_template('submit.html', thanks=True)\n        except:\n            #show a message if there is an error\n            return render_template('submit.html', error=True)\nThe second function is for viewing the messages. The site first grabs n_messages random messages from the databse using the random_messages function, then converts the dataframe to a list of tuples so that it can be parsed with jinja. This is then passed to the render_template function so the information can be used when displaying the site:\n@app.route('/view/')\ndef view():\n    n_messages = 5\n    #get n messages as a dataframe\n    df = random_messages(n_messages)\n    #convert dataframe to list of tuples for jinja\n    messages = list(df.itertuples(index=False, name=None))\n    #return as an argument\n    return render_template('view.html', messages=messages)\n\n\nTemplates\nThe first template is for submitting messages. In the first few lines extends base.html for the header. The following block is just a form to collect the handle and message data. Once the form is completed, it will display either the thank you or error message.\n{% extends 'base.html' %}\n\n{% block header %}\n  &lt;h1&gt;{% block title %}Submit{% endblock %}&lt;/h1&gt;\n{% endblock %}\n\n{% block content %}\n  &lt;!-- form with fields for message and handle --&gt;\n  &lt;form method=\"post\"&gt;\n      &lt;label for=\"message\"&gt;Your message:&lt;/label&gt;\n      &lt;input type=\"text\" name=\"message\" id=\"message\"&gt;\n      &lt;br&gt;&lt;br&gt;\n      &lt;label for=\"handle\"&gt;Your name or handle:&lt;/label&gt;\n      &lt;input type=\"text\" name=\"handle\" id=\"handle\"&gt;\n      &lt;br&gt;&lt;br&gt;\n      &lt;input type=\"submit\" value=\"Submit message\"&gt;\n  &lt;/form&gt;\n  &lt;!-- show messages after submitting --&gt;\n  {% if thanks %}\n  &lt;br&gt;\n  &lt;b&gt;Thanks for your submission!&lt;/b&gt;\n  {% elif error %}\n  &lt;b&gt;There was an error processing your submssion.&lt;/b&gt;\n  {% endif %}\n{% endblock %}\nThis is what it looks like in action: \nThe second template is for viewing messages. It takes the messages argument that was passed into the render_template function and iterates through each of its elements, indexing to access the handle and message separately so that they can be displayed in different lines:\n{% extends 'base.html' %}\n\n{% block header %}\n  &lt;h1&gt;{% block title %}Messages{% endblock %}&lt;/h1&gt;\n{% endblock %}\n\n{% block content %}\n&lt;!-- loop over all messages in the array --&gt;\n  {% for message in messages %}\n  &lt;!-- index to get message --&gt;\n  {{message[1]}}&lt;br&gt;\n  &lt;!-- index to get author --&gt;\n  &lt;i&gt;~{{message[0]}}&lt;/i&gt;&lt;br&gt;\n  &lt;br&gt;\n  {% endfor %}\n\n{% endblock %}\nThis is what it looks like in action:"
  },
  {
    "objectID": "posts/homework-5/index.html",
    "href": "posts/homework-5/index.html",
    "title": "Classifying Dogs and Cats with Keras",
    "section": "",
    "text": "In this blog post, I will be creating several convolutional neural networks with Keras to classify images of dogs and cats.\n\nSetup\nSince I am working in Google Colab, I need to upgrade Keras before importing it.\n\n!pip install keras --upgrade\n\nRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\nCollecting keras\n  Downloading keras-3.0.5-py3-none-any.whl (1.0 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 7.5 MB/s eta 0:00:00\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.25.2)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.7.0)\nCollecting namex (from keras)\n  Downloading namex-0.0.7-py3-none-any.whl (5.8 kB)\nRequirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.9.0)\nRequirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from keras) (0.1.8)\nRequirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.2.0)\nRequirement already satisfied: markdown-it-py&gt;=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich-&gt;keras) (3.0.0)\nRequirement already satisfied: pygments&lt;3.0.0,&gt;=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich-&gt;keras) (2.16.1)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py&gt;=2.2.0-&gt;rich-&gt;keras) (0.1.2)\nInstalling collected packages: namex, keras\n  Attempting uninstall: keras\n    Found existing installation: keras 2.15.0\n    Uninstalling keras-2.15.0:\n      Successfully uninstalled keras-2.15.0\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow 2.15.0 requires keras&lt;2.16,&gt;=2.15.0, but you have keras 3.0.5 which is incompatible.\nSuccessfully installed keras-3.0.5 namex-0.0.7\n\n\n\n#all necessary imports\nimport os\nimport numpy as np\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\nimport keras\nfrom keras import utils, layers, models\nimport tensorflow_datasets as tfds\nfrom tensorflow import expand_dims\nfrom tensorflow import data as tf_data\nfrom matplotlib import pyplot as plt\n\nWe can import an existing dataset from tfds and split it into training, validation, and test datasets.\n\n#create datasets\ntrain_ds, validation_ds, test_ds = tfds.load(\n    \"cats_vs_dogs\",\n    # 40% for training, 10% for validation, and 10% for test (the rest unused)\n    split=[\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"],\n    as_supervised=True,  # Include labels\n)\n\nprint(f\"Number of training samples: {train_ds.cardinality()}\")\nprint(f\"Number of validation samples: {validation_ds.cardinality()}\")\nprint(f\"Number of test samples: {test_ds.cardinality()}\")\n\nDownloading and preparing dataset 786.67 MiB (download: 786.67 MiB, generated: 1.04 GiB, total: 1.81 GiB) to /root/tensorflow_datasets/cats_vs_dogs/4.0.1...\nDataset cats_vs_dogs downloaded and prepared to /root/tensorflow_datasets/cats_vs_dogs/4.0.1. Subsequent calls will reuse this data.\nNumber of training samples: 9305\nNumber of validation samples: 2326\nNumber of test samples: 2326\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWARNING:absl:1738 images were corrupted and were skipped\n\n\n\n\n\nThe next two cells are for preparing the datasets by resizing them and splitting them into batches:\n\n#resize datasets\nresize_fn = keras.layers.Resizing(150, 150)\n\ntrain_ds = train_ds.map(lambda x, y: (resize_fn(x), y))\nvalidation_ds = validation_ds.map(lambda x, y: (resize_fn(x), y))\ntest_ds = test_ds.map(lambda x, y: (resize_fn(x), y))\n\n\nbatch_size = 64\n\ntrain_ds = train_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\nvalidation_ds = validation_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\ntest_ds = test_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\n\nWe can visualize some of the images from the dataset with matplotlib:\n\n#show 3 random cats, 3 random dogs\ndef showdata(ds):\n    plt.figure(figsize=(10, 6))\n    numdogs = 0\n    numcats = 0\n    for images, labels in ds.take(1):\n        for i in range(32):\n            if int(labels[i]) == 1 and numdogs &lt; 3: # if we need a dog...\n                ax = plt.subplot(2, 3, numdogs+4) # get the next slot on the 2nd row\n                plt.imshow(images[i].numpy().astype(\"uint8\"))\n                plt.axis(\"off\")\n                plt.title(\"dogs\")\n                numdogs += 1\n            if int(labels[i]) == 0 and numcats &lt; 3: # if we need a cat...\n                ax = plt.subplot(2, 3, numcats+1) # get the next slot on the 1st row\n                plt.imshow(images[i].numpy().astype(\"uint8\"))\n                plt.axis(\"off\")\n                plt.title(\"cats\")\n                numcats += 1\n            if numdogs == 3 and numcats == 3: # if we are done...\n                break\n\nshowdata(train_ds)\n\n\n\n\n\n\n\n\n\n\nModel 0: Baseline\nBy counting the total number of each label, we can construct a baseline model by simply guessing the most frequent label:\n\nlabels_iterator= train_ds.unbatch().map(lambda image, label: label).as_numpy_iterator()\n\ntotalcats = 0\ntotaldogs = 0\nfor label in labels_iterator:\n    if label == 0:\n        totalcats += 1\n    else:\n        totaldogs += 1\n\nprint(f\"{totalcats} cats and {totaldogs} dogs\")\nprint(f\"baseline: {round((totaldogs/(totalcats+totaldogs))*100,1)}% accuracy by always guessing dogs\")\n\n4637 cats and 4668 dogs\nbaseline: 50.2% accuracy by always guessing dogs\n\n\nAs you can see, this model is 50.2% accurate since 50.2% of the dataset is dogs.\n\n\nModel 1: Convolutional Neural Network\nWe first construct a basic CNN with Conv2D layers alternating with MaxPooling2D layers, ending with Flatten and Dense layers to get a single output. We can then compile the model, using BinaryCrossentropy as the loss function since we only have 2 categories.\nI experimented with various numbers of layers, adding more dropout layers, using bigger kernel sizes for the Conv2D layers, and using different activation functions, but ultimately this simple model worked best for me.\n\nmodel1 = models.Sequential([\n    layers.Input((150, 150, 3)), #input layer (same size as image)\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.Dropout(0.2),\n    layers.MaxPooling2D((2, 2)),\n    layers.Flatten(),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(1) #output layer\n])\n\nmodel1.compile(optimizer='adam',\n              loss=keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\nWe can now train the model with the training dataset:\n\nhistory1 = model1.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 7s 26ms/step - accuracy: 0.5072 - loss: 23.7128 - val_accuracy: 0.5284 - val_loss: 0.6591\nEpoch 2/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.5786 - loss: 0.6481 - val_accuracy: 0.5954 - val_loss: 0.6493\nEpoch 3/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.6766 - loss: 0.5742 - val_accuracy: 0.6414 - val_loss: 0.6812\nEpoch 4/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.7599 - loss: 0.4755 - val_accuracy: 0.6427 - val_loss: 0.6914\nEpoch 5/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.7908 - loss: 0.4220 - val_accuracy: 0.6187 - val_loss: 0.8573\nEpoch 6/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.8370 - loss: 0.3417 - val_accuracy: 0.6371 - val_loss: 1.0066\nEpoch 7/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.8563 - loss: 0.3158 - val_accuracy: 0.6354 - val_loss: 0.9163\nEpoch 8/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.8821 - loss: 0.2750 - val_accuracy: 0.6363 - val_loss: 1.0541\nEpoch 9/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.8942 - loss: 0.2455 - val_accuracy: 0.6144 - val_loss: 1.0995\nEpoch 10/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.9144 - loss: 0.2157 - val_accuracy: 0.6539 - val_loss: 1.0469\nEpoch 11/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.9254 - loss: 0.1752 - val_accuracy: 0.6410 - val_loss: 1.3221\nEpoch 12/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.9304 - loss: 0.1733 - val_accuracy: 0.6436 - val_loss: 1.4581\nEpoch 13/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.9456 - loss: 0.1370 - val_accuracy: 0.6182 - val_loss: 1.6986\nEpoch 14/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.9560 - loss: 0.1224 - val_accuracy: 0.6079 - val_loss: 1.7895\nEpoch 15/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.9653 - loss: 0.0999 - val_accuracy: 0.6367 - val_loss: 1.5307\nEpoch 16/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.9640 - loss: 0.1047 - val_accuracy: 0.6367 - val_loss: 1.7617\nEpoch 17/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.9674 - loss: 0.0958 - val_accuracy: 0.6436 - val_loss: 1.4820\nEpoch 18/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.9626 - loss: 0.1069 - val_accuracy: 0.6268 - val_loss: 1.6345\nEpoch 19/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.9701 - loss: 0.0881 - val_accuracy: 0.6384 - val_loss: 1.5326\nEpoch 20/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 1s 8ms/step - accuracy: 0.9711 - loss: 0.0826 - val_accuracy: 0.6453 - val_loss: 1.6826\n\n\n\nplt.plot(history1.history[\"accuracy\"], label = \"training\")\nplt.plot(history1.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\", xticks=np.arange(20))\nplt.legend()\n\n\n\n\n\n\n\n\nThe accuracy of model 1 stabilized at around 63%. This is 10% better than baseline, which is enough to be at least statistically significant. There is definitely overfitting, since the training accuracy rose to 97%, while the validation accuracy stayed at around 63%.\n\n\nModel 2: Data Augmentation\nWe can transform the images before using them to combat overfitting. One example of a transformation is the RandomFlip layer, which randomly horizontally/vertically reflects the image:\n\nfor image, _ in train_ds.take(1):\n    plt.figure(figsize=(10, 10))\n    first_image = image[0] #get the first image\n    ax = plt.subplot(3, 3, 1)\n    plt.imshow(first_image / 255) #plot the first image normally\n    plt.axis('off')\n    for i in range(8):\n        ax = plt.subplot(3, 3, i + 2) #for each slot...\n        augmented_image = layers.RandomFlip()(expand_dims(first_image, 0)) #transform the image\n        plt.imshow(augmented_image[0] / 255)\n        plt.axis('off')\n\n\n\n\n\n\n\n\nThe RandomRotation layer randomly rotates the image within a specified range of angles:\n\nfor image, _ in train_ds.take(1):\n    plt.figure(figsize=(10, 10))\n    first_image = image[0]\n    ax = plt.subplot(3, 3, 1)\n    plt.imshow(first_image / 255)\n    plt.axis('off')\n    for i in range(8):\n        ax = plt.subplot(3, 3, i + 2)\n        augmented_image = layers.RandomRotation(0.2)(expand_dims(first_image, 0))\n        plt.imshow(augmented_image[0] / 255)\n        plt.axis('off')\n\n\n\n\n\n\n\n\nWe can add these layers into the model to improve its accuracy:\n\nmodel2 = models.Sequential([\n    layers.Input((150, 150, 3)),\n    layers.RandomFlip(),\n    layers.RandomRotation(0.2),\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Flatten(),\n    layers.Dense(128, activation='relu'),\n    layers.Dropout(0.2),\n    layers.Dense(1)\n])\n\nmodel2.compile(optimizer='adam',\n              loss=keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\n\nhistory2 = model2.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 3s 12ms/step - accuracy: 0.5170 - loss: 11.1013 - val_accuracy: 0.5512 - val_loss: 0.6740\nEpoch 2/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.5585 - loss: 0.6711 - val_accuracy: 0.5542 - val_loss: 0.6639\nEpoch 3/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.5786 - loss: 0.6579 - val_accuracy: 0.5722 - val_loss: 0.6482\nEpoch 4/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.5945 - loss: 0.6492 - val_accuracy: 0.5911 - val_loss: 0.6394\nEpoch 5/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.6073 - loss: 0.6335 - val_accuracy: 0.6638 - val_loss: 0.6181\nEpoch 6/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.6223 - loss: 0.6354 - val_accuracy: 0.6397 - val_loss: 0.6086\nEpoch 7/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.6396 - loss: 0.6136 - val_accuracy: 0.6574 - val_loss: 0.6009\nEpoch 8/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.6477 - loss: 0.6088 - val_accuracy: 0.6660 - val_loss: 0.6037\nEpoch 9/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.6642 - loss: 0.5982 - val_accuracy: 0.6986 - val_loss: 0.5617\nEpoch 10/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.6732 - loss: 0.5796 - val_accuracy: 0.6978 - val_loss: 0.5572\nEpoch 11/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.6819 - loss: 0.5750 - val_accuracy: 0.6978 - val_loss: 0.5407\nEpoch 12/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.6998 - loss: 0.5546 - val_accuracy: 0.7283 - val_loss: 0.5391\nEpoch 13/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.7111 - loss: 0.5521 - val_accuracy: 0.7369 - val_loss: 0.5224\nEpoch 14/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.7036 - loss: 0.5467 - val_accuracy: 0.7283 - val_loss: 0.5237\nEpoch 15/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.7022 - loss: 0.5491 - val_accuracy: 0.7158 - val_loss: 0.5290\nEpoch 16/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.7213 - loss: 0.5261 - val_accuracy: 0.7180 - val_loss: 0.5111\nEpoch 17/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.7293 - loss: 0.5236 - val_accuracy: 0.7300 - val_loss: 0.5237\nEpoch 18/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.7316 - loss: 0.5124 - val_accuracy: 0.7386 - val_loss: 0.5002\nEpoch 19/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.7315 - loss: 0.5131 - val_accuracy: 0.7494 - val_loss: 0.5093\nEpoch 20/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.7302 - loss: 0.5144 - val_accuracy: 0.7627 - val_loss: 0.5045\n\n\n\nplt.plot(history2.history[\"accuracy\"], label = \"training\")\nplt.plot(history2.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\", xticks=np.arange(20))\nplt.legend()\n\n\n\n\n\n\n\n\nThe validation accuracy near the end of training was around 75%. This is a 10% increase from model1, which is definitely a lot better, but still not as good as a human. The graph shows significantly less overfitting, with validation and training data showing roughly the same accuracy during each epoch.\n#Model 3: Data Preprocessing We can refine the model further by adding a preprocessing layer to rescale the pixel values to be between -1 and 1, rather than 0 and 255.\n\ni = keras.Input(shape=(150, 150, 3))\nscale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=-1)\nx = scale_layer(i)\npreprocessor = keras.Model(inputs=i, outputs =x)\n\nmodel3 = models.Sequential([\n    preprocessor,\n    layers.RandomFlip(),\n    layers.RandomRotation(0.2),\n\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n\n    layers.Conv2D(32, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n\n    layers.Flatten(),\n    layers.Dense(128, activation='relu'),\n    layers.Dropout(0.2),\n    layers.Dense(1, activation=\"sigmoid\")\n])\n\nmodel3.compile(optimizer='adam',\n              loss=keras.losses.BinaryCrossentropy(),\n              metrics=['accuracy'])\n\n\nhistory3 = model3.fit(train_ds,\n                     epochs=25,\n                     validation_data=validation_ds)\n\nEpoch 1/25\n146/146 ━━━━━━━━━━━━━━━━━━━━ 4s 14ms/step - accuracy: 0.5635 - loss: 0.6873 - val_accuracy: 0.6509 - val_loss: 0.6286\nEpoch 2/25\n146/146 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.6411 - loss: 0.6364 - val_accuracy: 0.6978 - val_loss: 0.5864\nEpoch 3/25\n146/146 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.6734 - loss: 0.6058 - val_accuracy: 0.7158 - val_loss: 0.5694\nEpoch 4/25\n146/146 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.6816 - loss: 0.5929 - val_accuracy: 0.7218 - val_loss: 0.5409\nEpoch 5/25\n146/146 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.7218 - loss: 0.5587 - val_accuracy: 0.7567 - val_loss: 0.5002\nEpoch 6/25\n146/146 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.7286 - loss: 0.5425 - val_accuracy: 0.7541 - val_loss: 0.5082\nEpoch 7/25\n146/146 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.7427 - loss: 0.5261 - val_accuracy: 0.7580 - val_loss: 0.4937\nEpoch 8/25\n146/146 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.7441 - loss: 0.5187 - val_accuracy: 0.7678 - val_loss: 0.4868\nEpoch 9/25\n146/146 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.7574 - loss: 0.5015 - val_accuracy: 0.7696 - val_loss: 0.4844\nEpoch 10/25\n146/146 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.7613 - loss: 0.4918 - val_accuracy: 0.7756 - val_loss: 0.4866\nEpoch 11/25\n146/146 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.7696 - loss: 0.4882 - val_accuracy: 0.7807 - val_loss: 0.4762\nEpoch 12/25\n146/146 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.7760 - loss: 0.4704 - val_accuracy: 0.7941 - val_loss: 0.4648\nEpoch 13/25\n146/146 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.7806 - loss: 0.4686 - val_accuracy: 0.7932 - val_loss: 0.4512\nEpoch 14/25\n146/146 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.7888 - loss: 0.4502 - val_accuracy: 0.8018 - val_loss: 0.4256\nEpoch 15/25\n146/146 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.7944 - loss: 0.4397 - val_accuracy: 0.8031 - val_loss: 0.4152\nEpoch 16/25\n146/146 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.8030 - loss: 0.4227 - val_accuracy: 0.8143 - val_loss: 0.4218\nEpoch 17/25\n146/146 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.7971 - loss: 0.4254 - val_accuracy: 0.8203 - val_loss: 0.4063\nEpoch 18/25\n146/146 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.8118 - loss: 0.4144 - val_accuracy: 0.8186 - val_loss: 0.3965\nEpoch 19/25\n146/146 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.8140 - loss: 0.4005 - val_accuracy: 0.8319 - val_loss: 0.3843\nEpoch 20/25\n146/146 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.8209 - loss: 0.3903 - val_accuracy: 0.8280 - val_loss: 0.3789\nEpoch 21/25\n146/146 ━━━━━━━━━━━━━━━━━━━━ 2s 11ms/step - accuracy: 0.8206 - loss: 0.3902 - val_accuracy: 0.8237 - val_loss: 0.3818\nEpoch 22/25\n146/146 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.8307 - loss: 0.3870 - val_accuracy: 0.8280 - val_loss: 0.3798\nEpoch 23/25\n146/146 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.8344 - loss: 0.3706 - val_accuracy: 0.8414 - val_loss: 0.3651\nEpoch 24/25\n146/146 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.8350 - loss: 0.3622 - val_accuracy: 0.8267 - val_loss: 0.3860\nEpoch 25/25\n146/146 ━━━━━━━━━━━━━━━━━━━━ 2s 12ms/step - accuracy: 0.8359 - loss: 0.3617 - val_accuracy: 0.8396 - val_loss: 0.3675\n\n\n\nplt.plot(history3.history[\"accuracy\"], label = \"training\")\nplt.plot(history3.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\", xticks=np.arange(25))\nplt.legend()\n\n\n\n\n\n\n\n\nThe validation accuracy stabilized at around 83%. There was 20% increase from model1, which is nearing human levels of accuracy. There is hardly any overfitting to be seen in the model.\n#Model 4: Transfer Learning We can further improve model accuracy by using an existing model (MobileNetV3Large in this case).\n\nIMG_SHAPE = (150, 150, 3)\nbase_model = keras.applications.MobileNetV3Large(input_shape=IMG_SHAPE,\n                                               include_top=False,\n                                               weights='imagenet')\nbase_model.trainable = False\n\ni = keras.Input(shape=IMG_SHAPE)\nx = base_model(i, training = False)\nbase_model_layer = keras.Model(inputs = i, outputs = x)\n\n\nmodel4 = models.Sequential([\n    layers.Input((150, 150, 3)),\n    layers.RandomFlip(),\n    layers.RandomRotation(0.2),\n    base_model_layer,\n    layers.GlobalMaxPooling2D(),\n    layers.Dropout(0.5),\n    layers.Dense(1)\n])\n\nmodel4.compile(optimizer='adam',\n              loss=keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\nmodel4.build(train_ds)\n\nmodel4.summary()\n\nModel: \"sequential_1\"\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ random_flip_1 (RandomFlip)           │ (None, 150, 150, 3)         │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ random_rotation_1 (RandomRotation)   │ (None, 150, 150, 3)         │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ functional_4 (Functional)            │ (None, 5, 5, 960)           │       2,996,352 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ global_max_pooling2d_1               │ (None, 960)                 │               0 │\n│ (GlobalMaxPooling2D)                 │                             │                 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dropout_1 (Dropout)                  │ (None, 960)                 │               0 │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense_1 (Dense)                      │ (None, 1)                   │             961 │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n\n\n\n Total params: 2,997,313 (11.43 MB)\n\n\n\n Trainable params: 961 (3.75 KB)\n\n\n\n Non-trainable params: 2,996,352 (11.43 MB)\n\n\n\nThe model summary shows that there are almost 3 million parameters in the model, most of them coming from the MobileNetV3Large layer. However, the total number of trainable parameters is 961, which is almost nothing compared to the previous models.\n\nhistory4 = model4.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 9s 24ms/step - accuracy: 0.6476 - loss: 3.6039 - val_accuracy: 0.9523 - val_loss: 0.2938\nEpoch 2/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 2s 15ms/step - accuracy: 0.8386 - loss: 1.3664 - val_accuracy: 0.9656 - val_loss: 0.1876\nEpoch 3/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 2s 15ms/step - accuracy: 0.8586 - loss: 0.9405 - val_accuracy: 0.9673 - val_loss: 0.1534\nEpoch 4/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 2s 15ms/step - accuracy: 0.8741 - loss: 0.6898 - val_accuracy: 0.9626 - val_loss: 0.1629\nEpoch 5/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 2s 15ms/step - accuracy: 0.8824 - loss: 0.5124 - val_accuracy: 0.9669 - val_loss: 0.1242\nEpoch 6/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 2s 15ms/step - accuracy: 0.8811 - loss: 0.4672 - val_accuracy: 0.9690 - val_loss: 0.1011\nEpoch 7/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 2s 15ms/step - accuracy: 0.8879 - loss: 0.3837 - val_accuracy: 0.9652 - val_loss: 0.0938\nEpoch 8/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 2s 15ms/step - accuracy: 0.8831 - loss: 0.3820 - val_accuracy: 0.9686 - val_loss: 0.0923\nEpoch 9/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 2s 15ms/step - accuracy: 0.8822 - loss: 0.3453 - val_accuracy: 0.9678 - val_loss: 0.0826\nEpoch 10/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 2s 16ms/step - accuracy: 0.8803 - loss: 0.3372 - val_accuracy: 0.9596 - val_loss: 0.1046\nEpoch 11/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 2s 14ms/step - accuracy: 0.8847 - loss: 0.3109 - val_accuracy: 0.9656 - val_loss: 0.0901\nEpoch 12/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 2s 14ms/step - accuracy: 0.8819 - loss: 0.3303 - val_accuracy: 0.9665 - val_loss: 0.0905\nEpoch 13/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 2s 15ms/step - accuracy: 0.8791 - loss: 0.3249 - val_accuracy: 0.9536 - val_loss: 0.1463\nEpoch 14/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 2s 15ms/step - accuracy: 0.8784 - loss: 0.3444 - val_accuracy: 0.9656 - val_loss: 0.1059\nEpoch 15/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 2s 14ms/step - accuracy: 0.8756 - loss: 0.3472 - val_accuracy: 0.9635 - val_loss: 0.1090\nEpoch 16/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 2s 14ms/step - accuracy: 0.8767 - loss: 0.3436 - val_accuracy: 0.9647 - val_loss: 0.1034\nEpoch 17/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 2s 14ms/step - accuracy: 0.8892 - loss: 0.3035 - val_accuracy: 0.9695 - val_loss: 0.0919\nEpoch 18/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 2s 15ms/step - accuracy: 0.8776 - loss: 0.3461 - val_accuracy: 0.9647 - val_loss: 0.0872\nEpoch 19/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 2s 15ms/step - accuracy: 0.8818 - loss: 0.3276 - val_accuracy: 0.9669 - val_loss: 0.0916\nEpoch 20/20\n146/146 ━━━━━━━━━━━━━━━━━━━━ 2s 15ms/step - accuracy: 0.8734 - loss: 0.3523 - val_accuracy: 0.9630 - val_loss: 0.1072\n\n\n\nplt.plot(history4.history[\"accuracy\"], label = \"training\")\nplt.plot(history4.history[\"val_accuracy\"], label = \"validation\")\nplt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\", xticks=np.arange(20))\nplt.legend()\n\n\n\n\n\n\n\n\nThe model has a constant 96% accuracy right out of the box, which is far better than model1, with almost 30% better validation accuracy. For some reason, there is the opposite of overfitting happening, with the validation accuracy stabilizing at 10% better than the training accuracy.\n\n\nFinal results\n\nmodel4.evaluate(test_ds, verbose=2)\n\n37/37 - 1s - 16ms/step - accuracy: 0.9553 - loss: 0.1354\n\n\n[0.13536925613880157, 0.9552880525588989]\n\n\nSince model4 performed the best, I tested it against test_ds and got 95.5% accuracy, which around what I would expect based on the validation accuracy from training."
  }
]