[
  {
    "objectID": "posts/homework-2/index.html",
    "href": "posts/homework-2/index.html",
    "title": "Using Scrapy for Movie Recommendations",
    "section": "",
    "text": "In this tutorial, I will be creating a web crawler with scrapy, which I will use to scrape TMDB in order to make a primitive movie recommendation system by counting the number of shared actors with various films/TV shows. (The repo for this project can be found here)\n\nSetup\nStart by opening a terminal and typing the following commands (assuming you already have conda installed with scrapy):\nconda activate Environment_Name\nscrapy startproject TMDB_scraper\ncd TMDB_scraper\nThis will create a folder called TMDB_scraper in your current directory. Navigate to /TMND_scraper/TMND_scraper/settings.py and set the USER_AGENT variable to a user agent of your choosing. This will help bypass potential Error 403s later.\n\n\nWriting the scraper\nCreate a file in the /TMND_scraper/TMND_scraper/spiders directory called tmdb_spider.py. This is where the most of scraper behavior will be held.\nThen, add the following code block to the beginning of the file:\nimport scrapy\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n    def __init__(self, subdir=None, *args, **kwargs):\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\nThis defines the basic structure of the scraper. Next, add the following three methods to the file:\ndef parse(self, response):\n    \"\"\"\n    Parses TMDB movie pages.\n    Yields a scrapy.Request for the \"Full Cast & Crew\" page, with the parse_full_credit method specified in the callback argument.\n    Does not return any data.\n    \"\"\"\n    yield scrapy.Request(response.url+\"/cast/\", callback = self.parse_full_credits)\nAs the docstring states, this method takes a TMDB movie page and navigates to the “Full Cast & Crew” page by just appending “/cast/” to the url.\nThis page is then passed into the parse_full_credits method, defined below:\ndef parse_full_credits(self, response):\n    \"\"\"\n    Parses TMDB \"Full Cast & Crew\" pages.\n    Yields a scrapy.Request for the page of each cast member, with the parse_actor_page method specified in the callback argument.\n    Does not return any data.\n    \"\"\"\n    #Select only the Cast section\n    cast = response.css(\"ol.people.credits\")[0]\n    #Select actor names\n    for href in cast.css(\"p a::attr(href)\").getall():\n        yield scrapy.Request(response.urljoin(href), callback = self.parse_actor_page)\nThis method takes the “Full Cast & Crew” page and navigates to the cast members’ pages. It does this by selecting a css element containing the details of the cast members, then selecting all of the links in that section. Note the [0] index on the cast object! Leaving this off would select crew members as well, which is not what we want.\nThe pages from this method are passed into the parse_actor_pages method, defined below:\ndef parse_actor_page(self, response):\n    \"\"\"\n    Parses TMDB actor pages.\n    Yields a dictionary containing the actor's name keyed by \"actor\" and the movie/TV show keyed by \"movie_or_TV_name\" for each movie/TV show they have acted in.\n    \"\"\"\n    #Select actor name\n    actor = response.css(\"h2.title a::text\").get()\n\n    #Figure out which section is the \"Acting\" section by examining all the headers\n    acting_index = response.css(\"div.credits_list h3::text\").getall().index(\"Acting\")\n    #Select all acting roles\n    acting = response.css(\"div.credits_list table.card.credits\")[acting_index]\n    for title in acting.css(\"a.tooltip bdi::text\"):\n        yield {\"actor\": actor, \"movie_or_TV_name\": title.get()}\nFirst, this method finds the actor’s name on the page. This is easily accomplished by looking for the h2 css tag and taking the text under it.\nHowever, finding the sections with their acting roles is a little trickier. We first select all of the h3 headers under div.credits_list, looking for the one that says “Acting”. Then, we store the index and use it to select the corresponding table under div.credits_list. From there, we can easily select all of the titles. For the output, we yield a dictionary consisting of the actor’s name and the movie/TV series name for each acting credit.\n\n\nRunning the scraper\nOnce all of this is done, running the scraper is a relatively simple task. Simply type into the terminal:\nscrapy crawl tmdb_spider -o results.csv -a subdir=9323-ghost-in-the-shell\nOf course, you can replace the subdir argument with whatever movie you like. This command will produce a csv file in /TMDB_scraper/results.csv containing all of the actor/movie information for the given movie.\n\n\nReading the data\nWe can just use pandas to read the results from the results.csv file.\n\nimport pandas as pd\ndf = pd.read_csv(\"TMDB_scraper/results.csv\")\n\nWe can use pandas’s .value_counts() method to count the number of times each movie appears in the dataframe. This returns a series, so we need to do some manipulation to get it into a dataframe. We also rename the columns to make them look more professional for plotting.\n\ncounts = df[\"movie_or_TV_name\"].value_counts()\ncounts = counts.to_frame()\ncounts = counts.reset_index()\ncounts.rename(columns={\"movie_or_TV_name\": \"Movie or TV show name\",\"count\": \"Number of shared actors\"},inplace=True)\n\ncounts.head()\n\n\n\n\n\n\n\n\nMovie or TV show name\nNumber of shared actors\n\n\n\n\n0\nGhost in the Shell\n23\n\n\n1\nCowboy Bebop\n9\n\n\n2\nMonster\n8\n\n\n3\nBlack Jack\n6\n\n\n4\nGhost in the Shell: Stand Alone Complex\n6\n\n\n\n\n\n\n\n\n\nPlotting the data\nWe use plotly with the data, making a bar chart of the top 10 movies/TV shows sharing actors with Ghost in the Shell in this case.\n\nimport plotly.io as pio\npio.renderers.default=\"iframe\" #For rendering on the blog\nfrom plotly import express as px\n\nfig = px.bar(counts.head(10),\n                x=\"Movie or TV show name\",\n                y=\"Number of shared actors\",\n                title = \"Top 10 Movies/TV Shows with most actors in common with Ghost in the Shell\")\nfig.show()\n\n\n\n\nUnsurprisingly, Ghost in the Shell has the most actors in common with Ghost in the Shell. It’s also unsurprising that the various spin-offs of Ghost in the Shell also share a few actors. Cowboy Bebop, Monster, and Black Jack are all anime that ran during the same era, so it makes sense that they would share actors."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PIC 16B Blog",
    "section": "",
    "text": "Using Scrapy for Movie Recommendations\n\n\n\n\n\n\n\n\n\n\n\nJan 30, 2024\n\n\nEthan Buntaran\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 0\n\n\n\n\n\n\n\n\n\n\n\nJan 20, 2024\n\n\nEthan Buntaran\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/homework-0/index.html",
    "href": "posts/homework-0/index.html",
    "title": "Homework 0",
    "section": "",
    "text": "In this blog post, I will be describing how to make a simple line plot of the Palmer Penguins dataset.\n\nImporting packages and getting data\nTo start, we first import pandas in order to download and read the dataset.\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\n\nTo help with plotting, we import matplotlib and seaborn (a library for matplotlib).\n\nimport seaborn as sns \nfrom matplotlib import pyplot as plt\nimport numpy as np\n\n\n\nSelecting Data\nWe select only the subset of data that pertains to Adelie penguins.\n\nadelie = penguins[penguins[\"Species\"] == \"Adelie Penguin (Pygoscelis adeliae)\"]\n\n\n\nPlotting Data\nFinally, we can plot the data using seaborn’s lineplot function by specifying the dataset and the axes we wish to plot against.\n\nsns.lineplot(data = adelie, x = \"Flipper Length (mm)\", y = \"Body Mass (g)\").set(title=\"Flipper Length (mm) vs Body Mass (g) for Adelie Penguins\")"
  }
]