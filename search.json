[
  {
    "objectID": "posts/homework-3/index.html",
    "href": "posts/homework-3/index.html",
    "title": "A Simple Message Bank With Flask",
    "section": "",
    "text": "In this post I will be going through some of the functions and templates I used to create a simple online message bank using flask.\nThe repository for this project can be found here: https://github.com/ebuntaran/pic16b-mnist-demo/tree/fork\n\nHelper functions\nAll of these functions are defined in the app.py file, which contains most of the functionality of the website.\nThe first function (get_message_db) simply returns a database of messages if one exists. If no such database exists, it creates one with a table called messages with columns called handle and message using a SQL query .\ndef get_message_db():\n#check if there is a message database. if not...\n  try:\n      return g.message_db\n  except:\n    #create a new database\n    g.message_db = sqlite3.connect(\"messages_db.sqlite\")\n    #add a table with columns for handles and messages\n    cmd = 'CREATE TABLE IF NOT EXISTS messages (handle text(255), message text(255))'\n    cursor = g.message_db.cursor()\n    cursor.execute(cmd)\n    return g.message_db\nNow that we have a way to create/access a database, we need a way to add to the database. The insert_message message takes handle and message arguments, and uses these to add a row to the database using a SQL query:\ndef insert_message(handle, message):\n        #get database\n        db = get_message_db()\n        cursor = db.cursor()\n        #add a new row to the database with the given info with a sql command\n        cmd = \\\n        f\"\"\"\n        INSERT INTO messages (handle, message)\n        VALUES (\"{handle}\", \"{message}\");\n        \"\"\"\n        cursor.execute(cmd)\n        db.commit()\nNow all that’s left is a function to actually read the database. The random_messages accesses the database and selects at most n random messages from the database, returning them as a pandas dataframe:\ndef random_messages(n):\n        #select n random messages from the database with a sql command\n        cmd = \\\n        f\"\"\"\n        SELECT * FROM messages ORDER BY RANDOM() LIMIT {n};\n        \"\"\"\n        with get_message_db() as conn:\n            messages = pd.read_sql_query(cmd, conn)\n        #return as a dataframe\n        return messages\n\n\nSite functions\nNow that we have defined the helper functions, we can use them with the site rendering functions.\nThe first function is for the submission page. For GET requests, the site displays a form with input boxes for handle and message. For POST requests, the site uses the insert_message to add the information to the database, then displays a thank you message if it was successful:\n@app.route('/submit/', methods=['POST', 'GET'])\ndef submit():\n    if request.method == 'GET':\n        return render_template('submit.html')\n    else:\n        try:\n            #call the insert message function with the parameters given by the form\n            insert_message(request.form[\"handle\"], request.form[\"message\"])\n            #show a message if the request goes through\n            return render_template('submit.html', thanks=True)\n        except:\n            #show a message if there is an error\n            return render_template('submit.html', error=True)\nThe second function is for viewing the messages. The site first grabs n_messages random messages from the databse using the random_messages function, then converts the dataframe to a list of tuples so that it can be parsed with jinja. This is then passed to the render_template function so the information can be used when displaying the site:\n@app.route('/view/')\ndef view():\n    n_messages = 5\n    #get n messages as a dataframe\n    df = random_messages(n_messages)\n    #convert dataframe to list of tuples for jinja\n    messages = list(df.itertuples(index=False, name=None))\n    #return as an argument\n    return render_template('view.html', messages=messages)\n\n\nTemplates\nThe first template is for submitting messages. In the first few lines extends base.html for the header. The following block is just a form to collect the handle and message data. Once the form is completed, it will display either the thank you or error message.\n{% extends 'base.html' %}\n\n{% block header %}\n  &lt;h1&gt;{% block title %}Submit{% endblock %}&lt;/h1&gt;\n{% endblock %}\n\n{% block content %}\n  &lt;!-- form with fields for message and handle --&gt;\n  &lt;form method=\"post\"&gt;\n      &lt;label for=\"message\"&gt;Your message:&lt;/label&gt;\n      &lt;input type=\"text\" name=\"message\" id=\"message\"&gt;\n      &lt;br&gt;&lt;br&gt;\n      &lt;label for=\"handle\"&gt;Your name or handle:&lt;/label&gt;\n      &lt;input type=\"text\" name=\"handle\" id=\"handle\"&gt;\n      &lt;br&gt;&lt;br&gt;\n      &lt;input type=\"submit\" value=\"Submit message\"&gt;\n  &lt;/form&gt;\n  &lt;!-- show messages after submitting --&gt;\n  {% if thanks %}\n  &lt;br&gt;\n  &lt;b&gt;Thanks for your submission!&lt;/b&gt;\n  {% elif error %}\n  &lt;b&gt;There was an error processing your submssion.&lt;/b&gt;\n  {% endif %}\n{% endblock %}\nThis is what it looks like in action: \nThe second template is for viewing messages. It takes the messages argument that was passed into the render_template function and iterates through each of its elements, indexing to access the handle and message separately so that they can be displayed in different lines:\n{% extends 'base.html' %}\n\n{% block header %}\n  &lt;h1&gt;{% block title %}Messages{% endblock %}&lt;/h1&gt;\n{% endblock %}\n\n{% block content %}\n&lt;!-- loop over all messages in the array --&gt;\n  {% for message in messages %}\n  &lt;!-- index to get message --&gt;\n  {{message[1]}}&lt;br&gt;\n  &lt;!-- index to get author --&gt;\n  &lt;i&gt;~{{message[0]}}&lt;/i&gt;&lt;br&gt;\n  &lt;br&gt;\n  {% endfor %}\n\n{% endblock %}\nThis is what it looks like in action:"
  },
  {
    "objectID": "posts/homework-1/index.html",
    "href": "posts/homework-1/index.html",
    "title": "Analysis of Temperature Data for Countries",
    "section": "",
    "text": "#Included to make the plots interactive\nimport plotly.io as pio\npio.renderers.default=\"iframe\"\n\n\nSetting Up the Database\nMost of this code is taken from the lecture “Working with Datasets”:\nWe first clean up the dataframes, then add them as databases using sqlite3.\n\nwith open(\"database_setup.py\", 'r') as f:\n    print(f.read())\n\nimport sqlite3\nimport pandas as pd\n\n#Mostly taken from the lecture \"Working with Datasets\"\ndef prepare_df(df):\n    df = df.set_index(keys=[\"ID\", \"Year\"])\n    df = df.stack()\n    df = df.reset_index()\n    df = df.rename(columns = {\"level_2\"  : \"Month\" , 0 : \"Temp\"})\n    df[\"Month\"] = df[\"Month\"].str[5:].astype(int)\n    df[\"Temp\"]  = df[\"Temp\"] / 100\n    return(df)\n\nwith sqlite3.connect(\"temps.db\") as conn: # create a database in current directory called temps.db\n    temps_iter = pd.read_csv(\"temps.csv\", chunksize = 100000)\n    for i, df in enumerate(temps_iter):\n        df = prepare_df(df)\n        df.to_sql(\"temperatures\", conn, if_exists = \"replace\" if i == 0 else \"append\", index = False)\n    url = \"station-metadata.csv\"\n    stations = pd.read_csv(url)\n    stations.to_sql(\"stations\", conn, if_exists = \"replace\", index=False)\n    countries = pd.read_csv(\"https://raw.githubusercontent.com/mysociety/gaze/master/data/fips-10-4-to-iso-country-codes.csv\")\n    countries.to_sql(\"countries\", conn, if_exists = \"replace\", index=False)\n\n\n\n\nQuerying the Database\nThe vast majority of the query_climate_database function is a single SQL query. We select the relevant columns and only take rows satisfying the given constraints. Since country and station data is not in the temperatures database, we need to do some merges in order to get all the relevant information into one table.\n\nimport inspect\nfrom climate_database import query_climate_database\nprint(inspect.getsource(query_climate_database))\n\ndef query_climate_database(db_file, country, year_begin, year_end, month):\n    \"\"\"\n    Query a climate database for temperature information\n\n    Args:\n        db_file: sqlite3 database file\n        country (str): Country to take temperature data\n        year_begin (int): Year to begin taking data\n        year_end (int): Year to stop taking data\n        month (int): Month to take temperature data\n    \n    Returns:\n        df: Dataframe of temperature data at the provided country in the provided time range\n    \"\"\"\n\n    cmd = \\\n    f\"\"\"\n    SELECT S.name, S.latitude, S.longitude, C.Name, T.year, T.month, T.temp \n    FROM temperatures T\n    LEFT JOIN stations S ON T.id = S.id\n    INNER JOIN countries C ON SUBSTR(S.id,1,2) = C.\"FIPS 10-4\"\n    WHERE C.Name = \"{country}\" AND T.month = {month} AND T.year &gt;= {year_begin} AND T.year &lt;= {year_end}\n    \"\"\"\n    with sqlite3.connect(db_file) as conn:\n        df = pd.read_sql_query(cmd, conn)\n        df.rename(columns={\"Name\": \"Country\"},inplace=True)\n    return df\n\n\n\nHere is an example of this function in action:\n\nquery_climate_database(db_file = \"temps.db\",\n                       country = \"India\", \n                       year_begin = 1980, \n                       year_end = 2020,\n                       month = 1)\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nCountry\nYear\nMonth\nTemp\n\n\n\n\n0\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1980\n1\n23.48\n\n\n1\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1981\n1\n24.57\n\n\n2\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1982\n1\n24.19\n\n\n3\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1983\n1\n23.51\n\n\n4\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1984\n1\n24.81\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n3147\nDARJEELING\n27.050\n88.270\nIndia\n1983\n1\n5.10\n\n\n3148\nDARJEELING\n27.050\n88.270\nIndia\n1986\n1\n6.90\n\n\n3149\nDARJEELING\n27.050\n88.270\nIndia\n1994\n1\n8.10\n\n\n3150\nDARJEELING\n27.050\n88.270\nIndia\n1995\n1\n5.60\n\n\n3151\nDARJEELING\n27.050\n88.270\nIndia\n1997\n1\n5.70\n\n\n\n\n3152 rows × 7 columns\n\n\n\n\n\nPlotting the Data\nIn order to first filter out stations that do not have enough observation data, we add a column showing the number of years each sation has been providing data for using transform, which we can then use to filter out those rows.\nWith transform, we can estimate the yearly temperature increase using linear regression. Finally, we use plotly’s scatter_mapbox function to make a plot of the data.\n\nimport plotly\nfrom plotly import express as px\nimport numpy as np\nimport calendar\nfrom sklearn.linear_model import LinearRegression\n\ndef temperature_coefficient_plot(db_file, country, year_begin, year_end, month, min_obs, **kwargs):\n    \"\"\"\n    Use a climate database to make a geographic scatterplot\n\n    Args:\n        db_file: sqlite3 database file\n        country (str): Country to take temperature data\n        year_begin (int): Year to begin taking data\n        year_end (int): Year to stop taking data\n        month (int): Month to take temperature data\n        min_obs (int): Minimum number of years with recordings for stations\n        **kwargs: Additional arguments to be passed into scatter_mapbox \n    Returns:\n        fig: Geographic scatterplot of data\n    \"\"\"\n\n    #collect data\n    df = query_climate_database(db_file, country, year_begin, year_end, month)\n    \n    # we don't need the month and country columns anymore since month and country are arguments of the function\n    df = df.drop([\"Month\",\"Country\"], axis=1)\n\n    # filter out stations with range of years of operation less than min_obs\n    df[\"obs\"] = df.groupby([\"NAME\", \"LATITUDE\", \"LONGITUDE\"])[\"Year\"].transform(np.ptp)\n    df = df[df[\"obs\"]&gt;min_obs]\n\n    # defined in the lecture \"Advanced Data Manipulation II\": uses linear regression to calculate the estimated yearly increase\n    def coef(data_group):\n        x = data_group[[\"Year\"]] # 2 brackets because X should be a df\n        y = data_group[\"Temp\"]   # 1 bracket because y should be a series\n        LR = LinearRegression()\n        LR.fit(x, y)\n        return LR.coef_[0]\n\n    # add a column for the yearly increase\n    df = df.groupby([\"NAME\", \"LATITUDE\", \"LONGITUDE\"]).apply(coef)\n    df = df.reset_index()\n    df[0] = df[0].round(4)\n    df.rename(columns={0: \"Estimated Yearly Increase (°C)\"},inplace=True)\n\n    # create the plot with all the gathered information\n    fig = px.scatter_mapbox(df, \n                            title = f\"Estimates of yearly increase in temperature in {calendar.month_name[month]}&lt;br&gt;for stations in {country}, years {year_begin} - {year_end}\",\n                            hover_name = \"NAME\",\n                            lat = \"LATITUDE\",\n                            lon = \"LONGITUDE\", \n                            color = \"Estimated Yearly Increase (°C)\",\n                            **kwargs)\n    fig.update_coloraxes(cmid=0)\n    return fig\n\nWe can now call the functions with various arguments to see what the yearly increases in temperature are across each country.\n\ncolor_map = px.colors.diverging.RdGy_r\n\nfig1 = temperature_coefficient_plot(\"temps.db\", \"India\", 1980, 2020, 1, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\n\nfig1.show()\n\n\n\n\n\nfig2 = temperature_coefficient_plot(\"temps.db\", \"France\", 1980, 2020, 1, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\n\nfig2.show()\n\n\n\n\n\n\nPlotting Variation in Yearly Increase\nWe can see by the color scale that there can be significant variation of yearly increase within a country, but exactly how much is there? We can use the same data to make a box plot of yearly increases throughout a country, again using plotly.\n\nimport pandas as pd\n\ndef temperature_box_plot(db_file, countries, year_begin, year_end, month, min_obs):\n    \"\"\"\n    Use a climate database to make a box plot\n\n    Args:\n        db_file: sqlite3 database file\n        countries (list): Countries to take temperature data\n        year_begin (int): Year to begin taking data\n        year_end (int): Year to stop taking data\n        month (int): Month to take temperature data\n        min_obs (int): Minimum number of years with recordings for stations\n    Returns:\n        fig: Box plot of data\n    \"\"\"\n    df = pd.DataFrame()\n    #collect data\n    for country in countries:\n        df = pd.concat([df,query_climate_database(db_file, country, year_begin, year_end, month)], ignore_index = True)\n    \n    # we don't need the month column anymore since month is an argument of the function\n    df = df.drop([\"Month\"], axis=1)\n\n    # filter out stations with range of years of operation less than min_obs\n    df[\"obs\"] = df.groupby([\"NAME\", \"LATITUDE\", \"LONGITUDE\", \"Country\"])[\"Year\"].transform(np.ptp)\n    df = df[df[\"obs\"]&gt;min_obs]\n\n    # defined in the lecture \"Advanced Data Manipulation II\": uses linear regression to calculate the estimated yearly increase\n    def coef(data_group):\n        x = data_group[[\"Year\"]] # 2 brackets because X should be a df\n        y = data_group[\"Temp\"]   # 1 bracket because y should be a series\n        LR = LinearRegression()\n        LR.fit(x, y)\n        return LR.coef_[0]\n\n    # add a column for the yearly increase\n    df = df.groupby([\"NAME\", \"LATITUDE\", \"LONGITUDE\", \"Country\"]).apply(coef)\n    df = df.reset_index()\n    df[0] = df[0].round(4)\n    df.rename(columns={0: \"Estimated Yearly Increase (°C)\"},inplace=True)\n\n    # create the plot with all the gathered information\n    fig = px.box(df, \"Estimated Yearly Increase (°C)\", facet_col=\"Country\",\n                 title=f\"Variation in yearly increase in temperature in {calendar.month_name[month]}&lt;br&gt;for stations in {country}, years {year_begin} - {year_end}\",)\n    return fig\n\n\nfig3 = temperature_box_plot(\"temps.db\", [\"India\",\"China\",\"France\"], 1980, 2020, 1, min_obs = 10)\nfig3.show()\n\n\n\n\n\n\nIs Yearly Increase Correlated With Latitude?\nWe’ve seen that yearly increase can vary significantly within a country, so could this be tied to the latitude of the stations?\nTo answer this question, we first need another SQL querying function.\nquery_climate_database_long accesses the same information as query_climate_database, but it instead takes stations that are within tol degrees of a given longitude (long).\n\nfrom climate_database import query_climate_database_long\nimport inspect\nprint(inspect.getsource(query_climate_database_long))\n\ndef query_climate_database_long(db_file, long, tol, year_begin, year_end, month):\n    \"\"\"\n    Query a climate database for temperature information\n\n    Args:\n        db_file: sqlite3 database file\n        long (int): Longitude to take data from\n        tol (int): Tolerance for longitude value\n        year_begin (int): Year to begin taking data\n        year_end (int): Year to stop taking data\n        month (int): Month to take temperature data\n    \n    Returns:\n        df: Dataframe of temperature data near the provided longitude in the provided time range\n    \"\"\"\n\n    cmd = \\\n    f\"\"\"\n    SELECT S.name, S.latitude, S.longitude, T.year, T.month, T.temp \n    FROM temperatures T\n    LEFT JOIN stations S ON T.id = S.id\n    WHERE T.month = {month} AND T.year &gt;= {year_begin} AND T.year &lt;= {year_end} AND S.longitude &lt; {long+tol} AND S.longitude &gt; {long-tol}\n    \"\"\"\n    with sqlite3.connect(db_file) as conn:\n        df = pd.read_sql_query(cmd, conn)\n    return df\n\n\n\nWe can now use this function to gather data and plot it, again using plotly.\n\ndef temperature_longitude_plot(db_file, long, tol, year_begin, year_end, month, min_obs, **kwargs):\n    \"\"\"\n    Use a climate database to make a geographic scatterplot\n\n    Args:\n        db_file: sqlite3 database file\n        long (int): Longitude to take data from\n        tol (int): Tolerance for longitude value\n        year_begin (int): Year to begin taking data\n        year_end (int): Year to stop taking data\n        month (int): Month to take temperature data\n        min_obs (int): Minimum number of years with recordings for stations\n        **kwargs: Additional arguments to be passed into scatter \n    Returns:\n        fig: Scatterplot of data\n    \"\"\"\n    \n    #collect data\n    df = query_climate_database_long(db_file, long, tol, year_begin, year_end, month)\n    \n    # we don't need the month column anymore since month is an argument of the function\n    df = df.drop([\"Month\"], axis=1)\n\n    # filter out stations with range of years of operation less than min_obs\n    df[\"obs\"] = df.groupby([\"NAME\", \"LATITUDE\", \"LONGITUDE\"])[\"Year\"].transform(np.ptp)\n    df = df[df[\"obs\"]&gt;min_obs]\n\n    # defined in the lecture \"Advanced Data Manipulation II\": uses linear regression to calculate the estimated yearly increase\n    def coef(data_group):\n        x = data_group[[\"Year\"]] # 2 brackets because X should be a df\n        y = data_group[\"Temp\"]   # 1 bracket because y should be a series\n        LR = LinearRegression()\n        LR.fit(x, y)\n        return LR.coef_[0]\n\n    # add a column for the yearly increase\n    df = df.groupby([\"NAME\", \"LATITUDE\", \"LONGITUDE\"]).apply(coef)\n    df = df.reset_index()\n    df[0] = df[0].round(4)\n    df.rename(columns={0: \"Estimated Yearly Increase (°C)\", \"LATITUDE\": \"Latitude (°)\"},inplace=True)\n    \n    # create the plot with all the gathered information\n    fig = px.scatter(df, \n                    title = f\"Estimates of yearly increase in temperature in {calendar.month_name[month]}&lt;br&gt;for longitudes {long-tol}° to {long+tol}°, years {year_begin} - {year_end}\",\n                    hover_name = \"NAME\",\n                    x = \"Latitude (°)\",\n                    y = \"Estimated Yearly Increase (°C)\",\n                    **kwargs)\n    return fig\n\n\nfig4 = temperature_longitude_plot(\"temps.db\", 0, 5, 1980, 2020, 1, \n                                   min_obs = 10)\nfig4.show()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PIC 16B Blog",
    "section": "",
    "text": "A Simple Message Bank With Flask\n\n\n\n\n\n\n\n\n\n\n\nFeb 12, 2024\n\n\nEthan Buntaran\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Scrapy for Movie Recommendations\n\n\n\n\n\n\n\n\n\n\n\nJan 30, 2024\n\n\nEthan Buntaran\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of Temperature Data for Countries\n\n\n\n\n\n\n\n\n\n\n\nJan 27, 2024\n\n\nEthan Buntaran\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 0\n\n\n\n\n\n\n\n\n\n\n\nJan 20, 2024\n\n\nEthan Buntaran\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/homework-0/index.html",
    "href": "posts/homework-0/index.html",
    "title": "Homework 0",
    "section": "",
    "text": "In this blog post, I will be describing how to make a simple line plot of the Palmer Penguins dataset.\n\nImporting packages and getting data\nTo start, we first import pandas in order to download and read the dataset.\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\n\nTo help with plotting, we import matplotlib and seaborn (a library for matplotlib).\n\nimport seaborn as sns \nfrom matplotlib import pyplot as plt\nimport numpy as np\n\n\n\nSelecting Data\nWe select only the subset of data that pertains to Adelie penguins.\n\nadelie = penguins[penguins[\"Species\"] == \"Adelie Penguin (Pygoscelis adeliae)\"]\n\n\n\nPlotting Data\nFinally, we can plot the data using seaborn’s lineplot function by specifying the dataset and the axes we wish to plot against.\n\nsns.lineplot(data = adelie, x = \"Flipper Length (mm)\", y = \"Body Mass (g)\").set(title=\"Flipper Length (mm) vs Body Mass (g) for Adelie Penguins\")"
  },
  {
    "objectID": "posts/homework-2/index.html",
    "href": "posts/homework-2/index.html",
    "title": "Using Scrapy for Movie Recommendations",
    "section": "",
    "text": "In this tutorial, I will be creating a web crawler with scrapy, which I will use to scrape TMDB in order to make a primitive movie recommendation system by counting the number of shared actors with various films/TV shows. (The repo for this project can be found here)\n\nSetup\nStart by opening a terminal and typing the following commands (assuming you already have conda installed with scrapy):\nconda activate Environment_Name\nscrapy startproject TMDB_scraper\ncd TMDB_scraper\nThis will create a folder called TMDB_scraper in your current directory. Navigate to /TMND_scraper/TMND_scraper/settings.py and set the USER_AGENT variable to a user agent of your choosing. This will help bypass potential Error 403s later.\n\n\nWriting the scraper\nCreate a file in the /TMND_scraper/TMND_scraper/spiders directory called tmdb_spider.py. This is where the most of scraper behavior will be held.\nThen, add the following code block to the beginning of the file:\nimport scrapy\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n    def __init__(self, subdir=None, *args, **kwargs):\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\nThis defines the basic structure of the scraper. Next, add the following three methods to the file:\ndef parse(self, response):\n    \"\"\"\n    Parses TMDB movie pages.\n    Yields a scrapy.Request for the \"Full Cast & Crew\" page, with the parse_full_credit method specified in the callback argument.\n    Does not return any data.\n    \"\"\"\n    yield scrapy.Request(response.url+\"/cast/\", callback = self.parse_full_credits)\nAs the docstring states, this method takes a TMDB movie page and navigates to the “Full Cast & Crew” page by just appending “/cast/” to the url.\nThis page is then passed into the parse_full_credits method, defined below:\ndef parse_full_credits(self, response):\n    \"\"\"\n    Parses TMDB \"Full Cast & Crew\" pages.\n    Yields a scrapy.Request for the page of each cast member, with the parse_actor_page method specified in the callback argument.\n    Does not return any data.\n    \"\"\"\n    #Select only the Cast section\n    cast = response.css(\"ol.people.credits\")[0]\n    #Select actor names\n    for href in cast.css(\"p a::attr(href)\").getall():\n        yield scrapy.Request(response.urljoin(href), callback = self.parse_actor_page)\nThis method takes the “Full Cast & Crew” page and navigates to the cast members’ pages. It does this by selecting a css element containing the details of the cast members, then selecting all of the links in that section. Note the [0] index on the cast object! Leaving this off would select crew members as well, which is not what we want.\nThe pages from this method are passed into the parse_actor_pages method, defined below:\ndef parse_actor_page(self, response):\n    \"\"\"\n    Parses TMDB actor pages.\n    Yields a dictionary containing the actor's name keyed by \"actor\" and the movie/TV show keyed by \"movie_or_TV_name\" for each movie/TV show they have acted in.\n    \"\"\"\n    #Select actor name\n    actor = response.css(\"h2.title a::text\").get()\n\n    #Figure out which section is the \"Acting\" section by examining all the headers\n    acting_index = response.css(\"div.credits_list h3::text\").getall().index(\"Acting\")\n    #Select all acting roles\n    acting = response.css(\"div.credits_list table.card.credits\")[acting_index]\n    for title in acting.css(\"a.tooltip bdi::text\"):\n        yield {\"actor\": actor, \"movie_or_TV_name\": title.get()}\nFirst, this method finds the actor’s name on the page. This is easily accomplished by looking for the h2 css tag and taking the text under it.\nHowever, finding the sections with their acting roles is a little trickier. We first select all of the h3 headers under div.credits_list, looking for the one that says “Acting”. Then, we store the index and use it to select the corresponding table under div.credits_list. From there, we can easily select all of the titles. For the output, we yield a dictionary consisting of the actor’s name and the movie/TV series name for each acting credit.\n\n\nRunning the scraper\nOnce all of this is done, running the scraper is a relatively simple task. Simply type into the terminal:\nscrapy crawl tmdb_spider -o results.csv -a subdir=9323-ghost-in-the-shell\nOf course, you can replace the subdir argument with whatever movie you like. This command will produce a csv file in /TMDB_scraper/results.csv containing all of the actor/movie information for the given movie.\n\n\nReading the data\nWe can just use pandas to read the results from the results.csv file.\n\nimport pandas as pd\ndf = pd.read_csv(\"TMDB_scraper/results.csv\")\n\nWe can use pandas’s .value_counts() method to count the number of times each movie appears in the dataframe. This returns a series, so we need to do some manipulation to get it into a dataframe. We also rename the columns to make them look more professional for plotting.\n\ncounts = df[\"movie_or_TV_name\"].value_counts()\ncounts = counts.to_frame()\ncounts = counts.reset_index()\ncounts.rename(columns={\"movie_or_TV_name\": \"Movie or TV show name\",\"count\": \"Number of shared actors\"},inplace=True)\n\ncounts.head()\n\n\n\n\n\n\n\n\nMovie or TV show name\nNumber of shared actors\n\n\n\n\n0\nGhost in the Shell\n23\n\n\n1\nCowboy Bebop\n9\n\n\n2\nMonster\n8\n\n\n3\nBlack Jack\n6\n\n\n4\nGhost in the Shell: Stand Alone Complex\n6\n\n\n\n\n\n\n\n\n\nPlotting the data\nWe use plotly with the data, making a bar chart of the top 10 movies/TV shows sharing actors with Ghost in the Shell in this case.\n\nimport plotly.io as pio\npio.renderers.default=\"iframe\" #For rendering on the blog\nfrom plotly import express as px\n\nfig = px.bar(counts.head(10),\n                x=\"Movie or TV show name\",\n                y=\"Number of shared actors\",\n                title = \"Top 10 Movies/TV Shows with most actors in common with Ghost in the Shell\")\nfig.show()\n\n\n\n\nUnsurprisingly, Ghost in the Shell has the most actors in common with Ghost in the Shell. It’s also unsurprising that the various spin-offs of Ghost in the Shell also share a few actors. Cowboy Bebop, Monster, and Black Jack are all anime that ran during the same era, so it makes sense that they would share actors."
  }
]